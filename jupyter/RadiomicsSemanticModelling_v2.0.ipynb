{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('precision', 3)\n",
    "from itertools import compress\n",
    "import copy, sys, os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from pyirr import intraclass_correlation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, RepeatedStratifiedKFold, permutation_test_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve, confusion_matrix, make_scorer\n",
    "from scipy.stats import spearmanr, mannwhitneyu\n",
    "from compare_auc_delong import delong_roc_test\n",
    "import shap\n",
    "import pickle\n",
    "\n",
    "from featureSelection import featureSelection_correlation as featSelCorr\n",
    "\n",
    "# validation parameters\n",
    "n_splits = 10\n",
    "n_repeats = 100\n",
    "\n",
    "n_permutations = 100\n",
    "\n",
    "# blanket stop on drawing figures, so we don't overwrite by accident\n",
    "saveFigures = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_thresholds(yTrue, yScore, thresholds):\n",
    "    tnArr, fpArr, fnArr, tpArr = [], [], [], []\n",
    "    nSamples = len(yTrue)\n",
    "    for thresh in thresholds:\n",
    "        tn, fp, fn, tp = confusion_matrix(yTrue, yScore>thresh, labels=[0, 1]).ravel()\n",
    "        tnArr.append(tn)\n",
    "        fpArr.append(fp)\n",
    "        fnArr.append(fn)\n",
    "        tpArr.append(tp)\n",
    "    return np.array(tnArr), np.array(fpArr), np.array(fnArr), np.array(tpArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_fnr(y_true, y_pred, pt=0):\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred > pt).ravel()\n",
    "    return fn/(fn + tp)\n",
    "\n",
    "\n",
    "def calculate_fpr(y_true, y_pred, pt=0):\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred > pt).ravel()\n",
    "    return fp/(fp + tn)\n",
    "\n",
    "\n",
    "def calculate_net_benefit_score(y_true, y_pred, pt=0):\n",
    "    _, fp, _, tp = confusion_matrix(y_true, y_pred > pt).ravel()\n",
    "    net_benefit = (tp - fp * (pt / (1 - pt))) / len(y_true)\n",
    "    return net_benefit\n",
    "\n",
    "\n",
    "def unpack_scorers(cvr):\n",
    "\n",
    "    thresh_FNR, thresh_FPR, thresh_DCA, value_DCA, value_FNR, value_FPR = [], [], [], [], [], []\n",
    "    \n",
    "    for key, value in cvr.items():\n",
    "        if 'test_FNR' in key:\n",
    "            thresh_FNR.append(float(key.replace('test_FNR_','')))\n",
    "            value_FNR.append(np.mean(value))\n",
    "        if 'test_FPR' in key:\n",
    "            thresh_FPR.append(float(key.replace('test_FPR_','')))\n",
    "            value_FPR.append(np.mean(value))\n",
    "        if 'test_DCA' in key:\n",
    "            thresh_DCA.append(float(key.replace('test_DCA_','')))\n",
    "            value_DCA.append(np.mean(value))\n",
    "\n",
    "    idxFNR = np.argsort(thresh_FNR)\n",
    "    idxFPR = np.argsort(thresh_FPR)\n",
    "    idxDCA = np.argsort(thresh_DCA)\n",
    "\n",
    "    FNR = {'thresholds': [thresh_FNR[idx] for idx in idxFNR],\n",
    "           'values': [value_FNR[idx] for idx in idxFNR]}\n",
    "    \n",
    "    FPR = {'thresholds': [thresh_FPR[idx] for idx in idxFPR],\n",
    "           'values': [value_FPR[idx] for idx in idxFPR]}\n",
    "\n",
    "    DCA = {'thresholds': [thresh_DCA[idx] for idx in idxDCA],\n",
    "           'values': [value_DCA[idx] for idx in idxDCA]}\n",
    "    \n",
    "    return FNR, FPR, DCA\n",
    "\n",
    "# Make dictionary of scorers, each of which will compute one point on the FNR and FPR curves.\n",
    "# The dictionary key is used to keep track of the threshold value that was used.\n",
    "scorers = {}\n",
    "\n",
    "# Don't use 0 and 1 as endpoints as this causes numerical underflow.\n",
    "ptArr = np.round(np.linspace(0, 1, 101),2)\n",
    "ptArr[0] = np.round(0.0001,4)\n",
    "ptArr[-1] = np.round(0.9999,4)\n",
    "\n",
    "for pt in ptArr:\n",
    "    scorers['FNR_' + str(pt)] = make_scorer(calculate_fnr, pt = pt, needs_proba=True)\n",
    "    scorers['FPR_' + str(pt)] = make_scorer(calculate_fpr, pt = pt, needs_proba=True)\n",
    "    scorers['DCA_' + str(pt)] = make_scorer(calculate_net_benefit_score, pt = pt, needs_proba=True)\n",
    "\n",
    "# standard scorers    \n",
    "scorers['roc_auc'] = 'roc_auc'\n",
    "scorers['accuracy'] = 'accuracy'\n",
    "scorers['f1'] = 'f1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load two reader semantic data (for computing ICCs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "twoReaderFile = 'GG_MG.xlsx'\n",
    "\n",
    "# read spreadsheet\n",
    "df = pd.read_excel(twoReaderFile, sheet_name='GG_MG', engine='openpyxl')\n",
    "\n",
    "# remove features, as with the discovery/test data\n",
    "df.drop(['IndexLesion_GG', 'IndexLesionMG', 'GlobalStageGG', 'GlobalStageMG'], axis=1, inplace=True)\n",
    "\n",
    "# remove rows with missing data - need to check that this leaves the same patients for dfGG as in the discovery data set\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# split to each reader\n",
    "dfGG = df.filter(regex = 'GG|PID', axis = 1)\n",
    "dfMG = df.filter(regex='MG|PID', axis=1)\n",
    "\n",
    "# match column names by removing subscripts\n",
    "dfGG = dfGG.rename(columns=lambda x: x.replace('_GG','').replace('GG',''))\n",
    "dfMG = dfMG.rename(columns=lambda x: x.replace('_MG','').replace('MG',''))\n",
    "\n",
    "# change some column names to match the discovery/test data sets\n",
    "renameDict = {'LocIndexL':'AnatDev01',\n",
    "              'LocAnat':'AnatDev02',\n",
    "              'Division':'AnatDev03',\n",
    "              'DivisionLat':'AnatDev04',\n",
    "              'LesionSize':'MajorLengthIndex',\n",
    "              'SmoothCapsularBulgin':'SmoothCapsularBulging',\n",
    "              'UnsharpMargins':'UnsharpMargin',\n",
    "              'irregularContour':'IrregularContour',\n",
    "              'BlackEstrition':'BlackEstritionPeripFat',\n",
    "              'measurableECE':'MeasurableECE',\n",
    "              'retroprostaticAngleObl':'RetroprostaticAngleOblit'}\n",
    "dfGG.rename(renameDict, axis=1, inplace=True)\n",
    "dfMG.rename(renameDict, axis=1, inplace=True)\n",
    "\n",
    "# highsignalT1FS is missing from this spreadsheet, so fill in with default value.\n",
    "# Fortunately, this feature is not selected in the final model, but we need it there for compatibility.\n",
    "dfGG.loc[:, 'highsignalT1FS'] = 0\n",
    "dfMG.loc[:, 'highsignalT1FS'] = 0\n",
    "\n",
    "iccDict = {}\n",
    "for col in dfGG.drop(['PID', 'highsignalT1FS'], axis=1):\n",
    "    data = np.stack((dfGG[col], dfMG[col]), axis=1)\n",
    "    iccDict['semantic_' + col] = intraclass_correlation(data, \"twoway\", \"agreement\").value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and prepare radiomics data (discovery and test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "radiomicsFile = 'radiomicFeatures__202209271126.csv'\n",
    "\n",
    "dfRad = pd.read_csv(radiomicsFile)\n",
    "dfRad.drop(list(dfRad.filter(regex = 'source')), axis = 1, inplace = True)\n",
    "dfRad.drop(list(dfRad.filter(regex = 'diagnostics')), axis = 1, inplace = True)\n",
    "dfRad.drop(list(dfRad.filter(regex = 'histogram')), axis = 1, inplace = True)\n",
    "\n",
    "# remove feature sets we don't want to use and remove string from the one that is left\n",
    "dfRad.drop(list(dfRad.filter(regex = 'noNormalize|maskNormalize')), axis = 1, inplace = True)\n",
    "dfRad = dfRad.rename(columns=lambda x: x.replace('normalized_',''))\n",
    "\n",
    "# To match the semantic data file\n",
    "dfRad['StudyPatientName'] = dfRad['StudyPatientName'].str.replace('_',' ')\n",
    "\n",
    "# sensible prefix \n",
    "dfRad = dfRad.rename(columns=lambda x: x.replace('original','radiomics'))\n",
    "\n",
    "# split off the repro rows\n",
    "dfRep1 = dfRad.loc[dfRad.StudyPatientName.str.contains('rep'),:].copy()\n",
    "dfRep1['StudyPatientName'] = dfRep1['StudyPatientName'].str.replace(' repro','')\n",
    "dfRep1.sort_values('StudyPatientName', axis=0, inplace=True)\n",
    "dfRep1.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# remove repro from main data frame\n",
    "dfRad = dfRad.loc[~dfRad.StudyPatientName.str.contains('rep'),:]\n",
    "dfRad.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# main data rows for same patients as repro\n",
    "dfRep0 = dfRad.loc[dfRad['StudyPatientName'].isin(dfRep1['StudyPatientName'])].copy()\n",
    "dfRep0.sort_values('StudyPatientName', axis=0, inplace=True)\n",
    "dfRep0.reset_index(inplace=True, drop=True)\n",
    "\n",
    "for col in dfRep1.drop('StudyPatientName', axis=1):\n",
    "    data = np.stack((dfRep0[col], dfRep1[col]), axis=1)\n",
    "    iccDict[col] = intraclass_correlation(data, \"twoway\", \"agreement\").value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove non-reproducible radiomics features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-reproducible features\n",
    "iccThreshold = 0.75\n",
    "reproducibleFeatures = [key for key, value in iccDict.items() if value>iccThreshold and 'radiomics' in key]\n",
    "reproducibleFeatures.insert(0,'StudyPatientName')\n",
    "dfRad = dfRad[reproducibleFeatures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and prepare semantic data (discovery and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "discoveryFile = 'discovery.csv'\n",
    "externalTestFile = 'external.csv'\n",
    "    \n",
    "# load data\n",
    "dfTrain = pd.read_csv(discoveryFile)\n",
    "dfTest  = pd.read_csv(externalTestFile)\n",
    "\n",
    "# drop features we are not going to use for classification\n",
    "dfTrain.drop(['Gleason biopsy','TumorGradeMRI'], inplace=True, axis=1)\n",
    "dfTest.drop(['Gleason biopsy','TumorGradeMRI'], inplace=True, axis=1)\n",
    "\n",
    "# change this feature to have a more descriptive name\n",
    "dfTrain.insert(1, 'Gleason>(3+4)', dfTrain['GleasonBinary'])\n",
    "dfTest.insert(1, 'Gleason>(3+4)', dfTest['GleasonBinary'])\n",
    "dfTrain.drop('GleasonBinary', axis=1, inplace=True)\n",
    "dfTest.drop('GleasonBinary', axis=1, inplace=True)\n",
    "\n",
    "# make these features binary 0/1\n",
    "toBinary = ['SmoothCapsularBulging' ,'CapsularDisruption', 'UnsharpMargin', 'IrregularContour', 'BlackEstritionPeripFat', 'MeasurableECE', 'RetroprostaticAngleOblit', 'highsignalT1FS']\n",
    "for tb in toBinary:\n",
    "    dfTrain[tb]  = dfTrain[tb].map(dict(YES=1, NO=0))\n",
    "    dfTest[tb] = dfTest[tb].map(dict(YES=1, NO=0))\n",
    "\n",
    "# one-hot encode PIRADS\n",
    "dfTrain.insert(4, 'PIRADS_4', (dfTrain['IndLesPIRADS_V2']==4).astype(int))\n",
    "dfTrain.insert(4, 'PIRADS_5', (dfTrain['IndLesPIRADS_V2']==5).astype(int))\n",
    "dfTrain.drop('IndLesPIRADS_V2', axis=1, inplace=True)\n",
    "dfTest.insert(4, 'PIRADS_4', (dfTest['IndLesPIRADS_V2']==4).astype(int))\n",
    "dfTest.insert(4, 'PIRADS_5', (dfTest['IndLesPIRADS_V2']==5).astype(int))\n",
    "dfTest.drop('IndLesPIRADS_V2', axis=1, inplace=True)\n",
    "\n",
    "# log transform some features to get rid of long tail\n",
    "dfTrain.ProstateVolume = np.log(dfTrain.ProstateVolume)\n",
    "dfTest.ProstateVolume = np.log(dfTest.ProstateVolume)\n",
    "dfTrain.PSA = np.log(dfTrain.PSA)\n",
    "dfTest.PSA = np.log(dfTest.PSA)\n",
    "\n",
    "# is missing in test and training, so replace both with median from the training data\n",
    "psaTrainMedian = np.nanmedian(np.array(dfTrain.PSA))\n",
    "dfTrain.PSA.fillna(psaTrainMedian, inplace=True)\n",
    "dfTest.PSA.fillna(psaTrainMedian, inplace=True)\n",
    "\n",
    "# this feature is not selected in the semantic model, so this has no effect\n",
    "# fill in with the most common value\n",
    "dfTest.highsignalT1FS.fillna(0, inplace=True)\n",
    "\n",
    "# add interaction feature\n",
    "dfTrain['Gleason x mECE'] = dfTrain['Gleason>(3+4)']*dfTrain['MeasurableECE']\n",
    "dfTest['Gleason x mECE'] = dfTest['Gleason>(3+4)']*dfTest['MeasurableECE']\n",
    "\n",
    "dfTrain['Gleason x PIRADS_4'] = dfTrain['Gleason>(3+4)']*dfTrain['PIRADS_4']\n",
    "dfTrain['Gleason x PIRADS_5'] = dfTrain['Gleason>(3+4)']*dfTrain['PIRADS_5']\n",
    "dfTest['Gleason x PIRADS_4'] = dfTest['Gleason>(3+4)']*dfTest['PIRADS_4']\n",
    "dfTest['Gleason x PIRADS_5'] = dfTest['Gleason>(3+4)']*dfTest['PIRADS_5']\n",
    "\n",
    "# add string to names for easy manipulation of groups\n",
    "clinicalFeatures = ['Gleason>(3+4)', 'ProstateVolume', 'PSA', 'PIRADS_4', 'PIRADS_5', 'Gleason x PIRADS_5', 'Gleason x PIRADS_4']\n",
    "semanticFeatures = list(set(dfTrain.columns) - set(clinicalFeatures) - set(['PID', 'ECE_Pathology']))\n",
    "dfTrain = dfTrain.rename(columns=lambda x: 'clinical_' + x if x in clinicalFeatures else x)\n",
    "dfTest = dfTest.rename(columns=lambda x: 'clinical_' + x if x in clinicalFeatures else x)\n",
    "dfTrain = dfTrain.rename(columns=lambda x: 'semantic_' + x if x in semanticFeatures else x)\n",
    "dfTest = dfTest.rename(columns=lambda x: 'semantic_' + x if x in semanticFeatures else x)\n",
    "\n",
    "# merge radiomics \n",
    "dfTrain = dfTrain.merge(dfRad, left_on='PID', right_on='StudyPatientName')\n",
    "dfTest = dfTest.merge(dfRad, left_on='PID', right_on='StudyPatientName')\n",
    "dfTrain = dfTrain.drop(['PID', 'StudyPatientName'], axis=1)\n",
    "dfTest = dfTest.drop(['PID', 'StudyPatientName'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invert sense of the target variable and MeasurableECE\n",
    "\n",
    "i.e. target=1 implies \"no ECE\", target=0 implies \"ECE present\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch sense of the target variable and MeasureableECE\n",
    "dfTrain['noECE'] = 1 - dfTrain['ECE_Pathology']\n",
    "dfTest['noECE'] = 1 - dfTest['ECE_Pathology']\n",
    "dfTrain.drop('ECE_Pathology', axis=1, inplace=True)\n",
    "dfTest.drop('ECE_Pathology', axis=1, inplace=True)\n",
    "\n",
    "dfTrain['semantic_noMeasECE'] = 1 - dfTrain['semantic_MeasurableECE']\n",
    "dfTest['semantic_noMeasECE'] = 1 - dfTest['semantic_MeasurableECE']\n",
    "dfTrain.drop('semantic_MeasurableECE', axis=1, inplace=True)\n",
    "dfTest.drop('semantic_MeasurableECE', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model fitting function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fitModel(dfTrain, dfTest, pipeline, label=None, pickleFile=None, coefDisplayFunction=None, keepRegex=None, removeList = None, permutationTest=False):\n",
    "\n",
    "    if pickleFile is not None and os.path.exists(pickleFile):\n",
    "        with open(pickleFile, 'rb') as handle:\n",
    "            out = pickle.load(handle)\n",
    "\n",
    "    else:\n",
    "    \n",
    "        # reproducible execution\n",
    "        seed = 42\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # get training and test data\n",
    "        XTrain = dfTrain.drop('noECE', axis=1).copy()\n",
    "        XTest = dfTest.drop('noECE', axis=1).copy()\n",
    "        yTrain = dfTrain['noECE'].copy()\n",
    "        yTest = dfTest['noECE'].copy()\n",
    "\n",
    "        # keep features as indicated\n",
    "        if keepRegex is not None:\n",
    "            XTrain = XTrain.filter(regex = keepRegex)\n",
    "            XTest = XTest.filter(regex = keepRegex)\n",
    "\n",
    "        # remove listed features\n",
    "        if removeList is not None:\n",
    "            for item in removeList:\n",
    "                if item in XTrain:\n",
    "                    XTrain.drop(item, axis=1, inplace=True)\n",
    "                    XTest.drop(item, axis=1, inplace=True)\n",
    "\n",
    "        # make sure correlationSelector will not remove any clinical or semantic features\n",
    "        if pipeline.steps[0][0] == 'correlationSelector':\n",
    "            pipeline.steps[0][1].namedColumnsKeep = [x for x in XTrain.columns if 'clinical' in x or 'semantic' in x]\n",
    "\n",
    "        # drop low variance radiomics features (in practice this only includes radiomics_glcm_Idmn)\n",
    "        CoV = pd.DataFrame(XTrain.apply(lambda x: np.std(x)/np.mean(x), axis=0))\n",
    "        lowVarianceFeatures = CoV.loc[np.abs(CoV.loc[:,0])<0.01,:].index.values.tolist()\n",
    "        lowVarianceFeatures = [x for x in lowVarianceFeatures if 'clinical' not in x and 'semantic' not in x]\n",
    "        XTrain.drop(lowVarianceFeatures, axis = 1, inplace = True)\n",
    "        XTest.drop(lowVarianceFeatures, axis = 1, inplace = True)\n",
    "\n",
    "        # fit to all data\n",
    "        pipeline.fit(XTrain, yTrain)\n",
    "\n",
    "        # cross-validate\n",
    "        outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "        cv_result = cross_validate(pipeline, \n",
    "                                   XTrain, \n",
    "                                   yTrain, \n",
    "                                   cv=outer_cv, \n",
    "                                   scoring=scorers,\n",
    "                                   return_estimator=True, \n",
    "                                   verbose=0, \n",
    "                                   n_jobs=-1)\n",
    "\n",
    "        # print CV scores\n",
    "        AUC_CV = np.mean(cv_result['test_roc_auc'])\n",
    "        Accuracy_CV = np.mean(cv_result['test_accuracy'])\n",
    "        F1_CV = np.mean(cv_result['test_f1'])\n",
    "\n",
    "        FNR, FPR, DCA = unpack_scorers(cv_result)\n",
    "\n",
    "        # permutation testing\n",
    "        if permutationTest:\n",
    "            outer_cv.n_repeats = 1\n",
    "\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "            #\n",
    "            scoreDirect, perm_scores, pValueDirect = permutation_test_score(pipeline, \n",
    "                                                                            XTrain, \n",
    "                                                                            yTrain, \n",
    "                                                                            scoring=\"roc_auc\",\n",
    "                                                                            cv=outer_cv, \n",
    "                                                                            n_permutations=n_permutations,\n",
    "                                                                            verbose=0, n_jobs=-1)\n",
    "            #\n",
    "            warnings.simplefilter('default')\n",
    "            os.environ[\"PYTHONWARNINGS\"] = 'default'\n",
    "\n",
    "            # pValueDirect is computed using scoreDirect and assumes only one outer CV run\n",
    "            # We have used repeated outer CV, so the following code correctly computes the p-value of our repeated CV performance estimate\n",
    "            # Actually, it doesn't seem to make much difference, so am relaxed about that.\n",
    "\n",
    "            p_values = []\n",
    "            scores_roc_auc = np.mean(np.reshape(cv_result['test_roc_auc'], (n_repeats, -1)), axis=1)\n",
    "            for score in scores_roc_auc:\n",
    "                p_values.append((np.count_nonzero(perm_scores >= score) + 1) / (n_permutations + 1))\n",
    "            pValue_CV = np.mean(p_values)\n",
    "            print('p-value       = ' + str(pValue_CV.round(4)))\n",
    "        else:\n",
    "            pValue_CV = None\n",
    "\n",
    "\n",
    "        # get scores and predicted class info\n",
    "        test_score = pipeline.predict_proba(XTest)[:, 1]\n",
    "        test_class = pipeline.predict(XTest)\n",
    "\n",
    "        # test scores from main model\n",
    "        testAUROC = roc_auc_score(yTest, test_score)\n",
    "        testAccuracy = accuracy_score(yTest, test_class)\n",
    "        testF1 = f1_score(yTest, test_class)\n",
    "\n",
    "        pValueTest = mannwhitneyu(test_score[yTest==0], test_score[yTest==1], alternative='two-sided').pvalue\n",
    "\n",
    "        out = {'label':label,\n",
    "               'test_AUC':testAUROC,\n",
    "               'test_score':test_score,\n",
    "               'test_class':test_class,\n",
    "               'test_accuracy':testAccuracy,\n",
    "               'test_f1':testF1,\n",
    "               'test_pValue':pValueTest,\n",
    "               'AUC_CV':AUC_CV,\n",
    "               'Accuracy_CV':Accuracy_CV,\n",
    "               'F1_CV':F1_CV,\n",
    "               'pValue_CV':pValue_CV,\n",
    "               'FNR_CV':FNR,\n",
    "               'FPR_CV':FPR,\n",
    "               'DCA_CV':DCA,\n",
    "               'XTrain':XTrain.copy(),\n",
    "               'XTest':XTest.copy(),\n",
    "               'yTrain':yTrain.copy(),\n",
    "               'yTest':yTest.copy(),\n",
    "               'pipeline':pipeline,\n",
    "               'cv_result':cv_result\n",
    "              }\n",
    "\n",
    "    print('AUCROC   (CV) = ' + str(out['AUC_CV'].round(3)) + ' \\u00B1 ' + str(np.round(np.std(out['cv_result']['test_roc_auc']),3)))\n",
    "    print('Accuracy (CV) = ' + str(out['Accuracy_CV'].round(3)) + ' \\u00B1 ' + str(np.round(np.std(out['cv_result']['test_accuracy']),3)))\n",
    "    print('F1       (CV) = ' + str(out['F1_CV'].round(3)) + ' \\u00B1 ' + str(np.round(np.std(out['cv_result']['test_f1']),3)))\n",
    "    \n",
    "    \n",
    "    # print the test performance metrics\n",
    "    print('\\nAUCROC   (test) = ' + str(np.round(out['test_AUC'],3)))\n",
    "    print('Accuracy (test) = ' + str(np.round(out['test_accuracy'],3)))\n",
    "    print('F1       (test) = ' + str(np.round(out['test_f1'],3)))\n",
    "    print('p-value  (test) = ' + str(np.round(out['test_pValue'], 6)))\n",
    "\n",
    "    if coefDisplayFunction is not None:\n",
    "        dfCoefResults = coefDisplayFunction(out['pipeline'], out['XTrain'].columns, out['cv_result'])\n",
    "\n",
    "        \n",
    "    if pickleFile is not None:\n",
    "        with open(pickleFile, 'wb') as handle:\n",
    "            pickle.dump(out, handle)\n",
    "\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three logistic regression pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic logistic regression, no regularisation\n",
    "pipelineLRsimple = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                                   ('lr', LogisticRegression(penalty='none'))])\n",
    "\n",
    "# logistic regression with lasso\n",
    "pipelineLRlasso = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                                  ('lr', LogisticRegressionCV(Cs=20, \n",
    "                                                              cv=10, \n",
    "                                                              solver=\"liblinear\",\n",
    "                                                              max_iter=10000, \n",
    "                                                              penalty='l1',\n",
    "                                                              random_state=42))])\n",
    "\n",
    "# logistic regression with lasso, preceded by correlation feature reduction\n",
    "# fitModel function will modify pipeline to make sure the clinical and semantic features are not affected by\n",
    "# correlation feature reduction\n",
    "pipelineCfrLRlasso = Pipeline([('correlationSelector', featSelCorr(threshold=0.9,\n",
    "                                                                   exact=False,\n",
    "                                                                   featureGroupHierarchy=['shape_MeshVolume',\n",
    "                                                                                          'shape',\n",
    "                                                                                          'firstorder'])),\n",
    "                               ('scaler', StandardScaler()),\n",
    "                               ('lr', GridSearchCV(estimator=LogisticRegression(solver=\"liblinear\", \n",
    "                                                                                max_iter=10000, \n",
    "                                                                                penalty='l1'), \n",
    "                                                   param_grid={'C':np.logspace(np.log10(0.05), np.log10(50), 20)}, \n",
    "                                                   cv=StratifiedKFold(n_splits=10), \n",
    "                                                   refit=True, \n",
    "                                                   verbose=0, \n",
    "                                                   scoring='neg_log_loss', \n",
    "                                                   n_jobs=1))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to display non-zero logistic regression coeffients for the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def coefDispFunLRsimple(pipeline, featureNames, dummy):\n",
    "    dfCoefResults = pd.DataFrame({'Feature':[x.replace('clinical_','') for x in featureNames], \n",
    "                                  'Coeff':np.squeeze(pipeline._final_estimator.coef_)})\n",
    "    dfCoefResults.sort_values(by='Coeff', key=abs, inplace=True, ascending=False)\n",
    "    display(dfCoefResults.style.hide_index())\n",
    "    print(dfCoefResults.to_string(index=False))\n",
    "\n",
    "def coefDispFunLRlasso(pipeline, featureNames, cv_result):\n",
    "\n",
    "    # get frequency that features are non-zero across the repeated cv splits\n",
    "    coef_cv = np.zeros((len(cv_result['estimator']), len(featureNames)))\n",
    "    for n, res in enumerate(cv_result['estimator']):\n",
    "        coef_cv[n, :] = res._final_estimator.coef_\n",
    "    coef_freq = np.sum(coef_cv != 0, axis=0) / (n_repeats * n_splits)\n",
    "\n",
    "    # put icc values in array for including in DataFrame\n",
    "    iccList = []\n",
    "    for feat in featureNames:\n",
    "        if feat in iccDict:\n",
    "            iccList.append(iccDict[feat])\n",
    "        else:\n",
    "            iccList.append('-')\n",
    "\n",
    "    # simplify feature names\n",
    "    featureNames = [x.replace('clinical_','') for x in featureNames]\n",
    "    featureNames = [x.replace('semantic_','') for x in featureNames]\n",
    "    featureNames = [x.replace('radiomics_','') for x in featureNames]\n",
    "            \n",
    "    # display sorted coefficients and selection frequency\n",
    "    coeffs = np.squeeze(pipeline._final_estimator.coef_)\n",
    "    dfCoefResults = pd.DataFrame({'Feature': featureNames, 'Coeff': coeffs, 'Freq': coef_freq, 'ICC':iccList})\n",
    "    dfCoefResults.sort_values(by=['Coeff', 'Freq'], key=abs, inplace=True, ascending=False)\n",
    "    display(dfCoefResults.loc[dfCoefResults.Coeff != 0, :].style.hide_index())\n",
    "    print(dfCoefResults.loc[dfCoefResults.Coeff != 0, :].to_string(index=False))\n",
    "\n",
    "def coefDispFunCfrLRlasso(pipeline, featureNames, cv_result):\n",
    "    \n",
    "    # get frequency that features are non-zero across the repeated cv splits\n",
    "    coef_cv = np.zeros((len(cv_result['estimator']), len(featureNames)))\n",
    "    fs_mask = np.zeros((len(cv_result['estimator']), len(featureNames)))\n",
    "    for n, res in enumerate(cv_result['estimator']):\n",
    "        fs_mask[n, :] = res.steps[0][1].mask_\n",
    "        coef_cv[n, res.steps[0][1].mask_] = res._final_estimator.best_estimator_.coef_\n",
    "    coef_freq = np.sum(coef_cv != 0, axis=0) / (n_repeats * n_splits)\n",
    "\n",
    "    # put icc values in array for including in DataFrame\n",
    "    iccList = []\n",
    "    for feat in featureNames:\n",
    "        if feat in iccDict:\n",
    "            iccList.append(iccDict[feat])\n",
    "        else:\n",
    "            iccList.append('-')\n",
    "\n",
    "    # simplify feature names\n",
    "    featureNames = [x.replace('clinical_','') for x in featureNames]\n",
    "    featureNames = [x.replace('semantic_','') for x in featureNames]\n",
    "    featureNames = [x.replace('radiomics_','') for x in featureNames]\n",
    "            \n",
    "    # display sorted coefficients and selection frequency\n",
    "    coeffs = np.zeros(len(featureNames))\n",
    "    coeffs[pipeline.steps[0][1].mask_] = np.squeeze(pipeline._final_estimator.best_estimator_.coef_)\n",
    "\n",
    "    dfCoefResults = pd.DataFrame({'Feature': featureNames, 'Coeff': coeffs, 'Freq': coef_freq, 'ICC':iccList})\n",
    "    dfCoefResults.sort_values(by=['Coeff', 'Freq'], key=abs, inplace=True, ascending=False)\n",
    "    display(dfCoefResults.loc[dfCoefResults.Coeff != 0, :].style.hide_index())\n",
    "    print(dfCoefResults.loc[dfCoefResults.Coeff != 0, :].to_string(index=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for SHAP beeswarm plots for the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def showShapLR(r, file=None, yAxisTrim=False):\n",
    "    standardiser = r['pipeline'].steps[0][1]\n",
    "\n",
    "    X = pd.DataFrame(columns=r['XTrain'].columns, data=standardiser.transform(r['XTrain'].copy()))\n",
    "    X = X.rename(columns=lambda x: x.replace('clinical_',''))\n",
    "    X = X.rename(columns=lambda x: x.replace('semantic_',''))\n",
    "    X = X.rename(columns=lambda x: x.replace('radiomics_',''))\n",
    "        \n",
    "    explainer = shap.explainers.Linear(r['pipeline']._final_estimator, X)\n",
    "    shap_values = explainer(X)\n",
    "    max_display = 1 + np.sum(r['pipeline']._final_estimator.coef_ != 0)\n",
    "    shap.plots.beeswarm(shap_values, max_display=max_display, show=False)\n",
    "\n",
    "    # trim off the \"and other features bit\" if necessary\n",
    "    if yAxisTrim:\n",
    "        plt.gcf().gca().set_ylim(0.5,plt.gcf().gca().get_ylim()[1])\n",
    "    else:\n",
    "        # make the space at the bottom the same as when we trim\n",
    "        plt.gcf().gca().set_ylim(-0.5,plt.gcf().gca().get_ylim()[1])\n",
    "\n",
    "    if file is not None and saveFigures:\n",
    "        plt.savefig(file, bbox_inches='tight')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def showShapLRcfr(r, file=None, yAxisTrim=False):\n",
    "\n",
    "    mask = r['pipeline'].steps[0][1].mask_\n",
    "    standardiser = r['pipeline'].steps[1][1]\n",
    "\n",
    "    X = r['XTrain'].loc[:,mask]\n",
    "    X = standardiser.transform(X)\n",
    "    X = pd.DataFrame(columns=r['XTrain'].columns[mask], data=X)\n",
    "    X = X.rename(columns=lambda x: x.replace('clinical_',''))\n",
    "    X = X.rename(columns=lambda x: x.replace('semantic_',''))\n",
    "    X = X.rename(columns=lambda x: x.replace('radiomics_',''))\n",
    "    \n",
    "    model = r['pipeline']._final_estimator.best_estimator_\n",
    "    explainer = shap.explainers.Linear(model, X)\n",
    "    shap_values = explainer(X)\n",
    "    max_display = 1 + np.sum(model.coef_ != 0)\n",
    "    shap.plots.beeswarm(shap_values, max_display=max_display, show=False)\n",
    "\n",
    "    # trim off the \"and other features bit\" if necessary\n",
    "    if yAxisTrim:\n",
    "        plt.gcf().gca().set_ylim(0.5,plt.gcf().gca().get_ylim()[1])\n",
    "    else:\n",
    "        # make the space at the bottom the same as when we trim\n",
    "        plt.gcf().gca().set_ylim(-0.5,plt.gcf().gca().get_ylim()[1])\n",
    "\n",
    "    if file is not None and saveFigures:\n",
    "        plt.savefig(file, bbox_inches='tight')\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models to four combinations of input features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runAllModels(permutationTest=False, removeList=None):\n",
    "\n",
    "    print('Clinical features\\n')\n",
    "    rc = fitModel(dfTrain,\n",
    "                  dfTest,\n",
    "                  pipelineLRsimple,\n",
    "                  label='Clinical',\n",
    "                  pickleFile=os.path.join('', 'pickles', 'clinical.pkl'),\n",
    "                  coefDisplayFunction=coefDispFunLRsimple, \n",
    "                  keepRegex='clinical', \n",
    "                  removeList=removeList,\n",
    "                  permutationTest=permutationTest)\n",
    "    showShapLR(rc, file=os.path.join('', 'figures', 'shap_clinical.pdf'))\n",
    "\n",
    "    print('\\n\\n\\nClinical + Semantic features\\n')\n",
    "    rcs = fitModel(dfTrain,\n",
    "                   dfTest,\n",
    "                   pipelineLRlasso,\n",
    "                   label='Clinical + Semantic',\n",
    "                   pickleFile=os.path.join('', 'pickles', 'clinical_semantic.pkl'),\n",
    "                   coefDisplayFunction=coefDispFunLRlasso, \n",
    "                   keepRegex='clinical|semantic', \n",
    "                   removeList=removeList,\n",
    "                   permutationTest=permutationTest)\n",
    "    showShapLR(rcs, yAxisTrim=True, file=os.path.join('', 'figures', 'shap_clinical_semantic.pdf'))\n",
    "\n",
    "\n",
    "    print('\\n\\n\\nClinical + Radiomic features\\n')\n",
    "    rcr = fitModel(dfTrain,\n",
    "                   dfTest,\n",
    "                   pipelineCfrLRlasso, \n",
    "                   label='Clinical + Radiomic',\n",
    "                   pickleFile=os.path.join('', 'pickles', 'clinical_radiomic.pkl'),\n",
    "                   coefDisplayFunction=coefDispFunCfrLRlasso, \n",
    "                   keepRegex='clinical|radiomic', \n",
    "                   removeList=removeList,\n",
    "                   permutationTest=permutationTest)\n",
    "    showShapLRcfr(rcr, yAxisTrim=True, file=os.path.join('', 'figures', 'shap_clinical_radiomic.pdf'))\n",
    "\n",
    "\n",
    "    print('\\n\\n\\nClinical + Semantic + Radiomic features\\n')\n",
    "    rcsr = fitModel(dfTrain,\n",
    "                    dfTest,\n",
    "                    pipelineCfrLRlasso, \n",
    "                    label='Clinical + Semantic + Radiomic',\n",
    "                    pickleFile=os.path.join('', 'pickles', 'clinical_semantic_radiomic.pkl'),\n",
    "                    coefDisplayFunction=coefDispFunCfrLRlasso, \n",
    "                    keepRegex='clinical|semantic|radiomic', \n",
    "                    removeList=removeList,\n",
    "                    permutationTest=permutationTest)\n",
    "    showShapLRcfr(rcsr, yAxisTrim=True, file=os.path.join('', 'figures', 'shap_clinical_semantic_radiomic.pdf'))\n",
    "\n",
    "    return {'clin':rc, 'clinSem':rcs, 'clinRad':rcr, 'clinSemRad':rcsr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove MeasurableECE and other unwanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeList = ['semantic_noMeasECE', 'semantic_Gleason x mECE', 'clinical_Gleason x PIRADS_5', 'clinical_Gleason x PIRADS_4']\n",
    "\n",
    "# generate the tabular outputs and SHAP plots\n",
    "models = runAllModels(permutationTest=False, removeList=removeList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testData = models['clin']['yTest']\n",
    "\n",
    "m1 = models['clin']\n",
    "m2 = models['clinSem']\n",
    "pValue = 10**delong_roc_test(testData, m1['test_score'], m2['test_score'])[0,0]\n",
    "print('\\nDelong comparing ' + m1['label'] + ' vs. ' + m2['label'] + '\\np = ' + str(np.round(pValue,3)))\n",
    "\n",
    "m1 = models['clin']\n",
    "m2 = models['clinRad']\n",
    "pValue = 10**delong_roc_test(testData, m1['test_score'], m2['test_score'])[0,0]\n",
    "print('\\nDelong comparing ' + m1['label'] + ' vs. ' + m2['label'] + '\\np = ' + str(np.round(pValue,3)))\n",
    "\n",
    "m1 = models['clin']\n",
    "m2 = models['clinSemRad']\n",
    "pValue = 10**delong_roc_test(testData, m1['test_score'], m2['test_score'])[0,0]\n",
    "print('\\nDelong comparing ' + m1['label'] + ' vs. ' + m2['label'] + '\\np = ' + str(np.round(pValue,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "def noMeasECEmodelCV(df, ptArr, n_splits=10, n_repeats=10):\n",
    "\n",
    "    X = df['semantic_noMeasECE'].copy()\n",
    "    y = df['noECE'].copy()\n",
    "\n",
    "    netBenefitCurve = np.zeros(ptArr.shape)\n",
    "    rocCurveX = np.zeros(ptArr.shape)\n",
    "    rocCurveY = np.zeros(ptArr.shape)\n",
    "    rocAUC = 0\n",
    "    count = 0\n",
    "    \n",
    "    validation = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "    \n",
    "    for train_index, test_index in validation.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_train, X_train, labels=[0,1]).ravel()\n",
    "        p0 = fn/(tn+fn)\n",
    "        p1 = tp/(tp+fp)\n",
    "        scoreTest = np.zeros(X_test.shape[0])\n",
    "        scoreTest[X_test==0] = p0\n",
    "        scoreTest[X_test==1] = p1\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix_thresholds(y_test, scoreTest, ptArr)\n",
    "        netBenefitCurve += (tp - fp * (ptArr / (1 - ptArr))) / len(y_test)\n",
    "        if np.all((tn+fp)>0) and np.all(tp+fn):\n",
    "            rocCurveXh = 1 - tn/(tn+fp)\n",
    "            rocCurveYh = tp/(tp+fn)\n",
    "            rocCurveX += rocCurveXh\n",
    "            rocCurveY += rocCurveYh\n",
    "            rocAUC += np.sum(-np.diff(rocCurveXh)*0.5*(rocCurveYh[0:-1] + rocCurveYh[1:]))\n",
    "            count += 1\n",
    "    netBenefitCurve /= count\n",
    "    rocCurveX /= count\n",
    "    rocCurveY /= count\n",
    "    rocAUC /= count\n",
    "    return netBenefitCurve, rocCurveX, rocCurveY, rocAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get threshold array pt from any model\n",
    "thresh = np.unique(models['clin']['DCA_CV']['thresholds'])\n",
    "thresh = np.append(thresh, 1e-6)\n",
    "thresh = np.append(thresh, 1-1e-6)\n",
    "thresh = np.sort(thresh)\n",
    "\n",
    "# cross-validate the noMeasECE model in the training data\n",
    "# netBenefitCurveCV, rocCurveX_CV, rocCurveY_CV, rocAUC_CV = noMeasECEmodelCV(dfTrain, thresh, n_splits=n_splits, n_repeats=n_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "ax = ax.ravel()\n",
    "\n",
    "# noMeasECE model derived from training data\n",
    "tn, fp, fn, tp = confusion_matrix(dfTrain['noECE'], dfTrain['semantic_noMeasECE']).ravel()\n",
    "p0mECE = fn/(tn+fn) # Pr(noECE=True | noMeasECE=False)\n",
    "p1mECE = tp/(tp+fp) # Pr(noECE=True | noMeasECE=True)\n",
    "\n",
    "scoreTrain = np.zeros(dfTrain.shape[0])\n",
    "scoreTrain[dfTrain['semantic_noMeasECE']==0] = p0mECE\n",
    "scoreTrain[dfTrain['semantic_noMeasECE']==1] = p1mECE\n",
    "tn, fp, fn, tp = confusion_matrix_thresholds(dfTrain['noECE'], scoreTrain, thresh)\n",
    "netBenefitTrain = (tp - fp * (thresh / (1 - thresh))) / len(dfTrain['noECE'])\n",
    "rocXtrain = 1 - tn/(tn+fp)\n",
    "rocYtrain = tp/(tp+fn)\n",
    "aucTrain = roc_auc_score(dfTrain['noECE'], scoreTrain)\n",
    "\n",
    "scoreTest = np.zeros(dfTest.shape[0])\n",
    "scoreTest[dfTest['semantic_noMeasECE']==0] = p0mECE\n",
    "scoreTest[dfTest['semantic_noMeasECE']==1] = p1mECE\n",
    "tn, fp, fn, tp = confusion_matrix_thresholds(dfTest['noECE'], scoreTest, thresh)\n",
    "netBenefitTest = (tp - fp * (thresh / (1 - thresh))) / len(dfTest['noECE'])\n",
    "rocXtest = 1 - tn/(tn+fp)\n",
    "rocYtest = tp/(tp+fn)\n",
    "\n",
    "# discovery data plots\n",
    "\n",
    "# ROC\n",
    "for r in models.values():\n",
    "    ax[0].plot(r['FPR_CV']['values'], \n",
    "               [1-x for x in r['FNR_CV']['values']], \n",
    "               label = 'AUC = ' + str(np.round(r['AUC_CV'],3)) + ' ' + r['label'])\n",
    "\n",
    "ax[0].plot(rocXtrain, rocYtrain, label='AUC = ' + str(np.round(aucTrain,3)) + ' MeasurableECE')\n",
    "# ax[0].plot(rocCurveX_CV, rocCurveY_CV, label='AUC = ' + str(np.round(rocAUC_CV,3)) + ' MeasurableECE CV')\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('1 - Specificity', fontsize=14)\n",
    "ax[0].set_ylabel('Sensitivity', fontsize=14)\n",
    "ax[0].set_title('ROC', fontsize=16, fontweight='bold')\n",
    "ax[0].text(-0.3, 0.5, 'Discovery data, cross-validated', fontsize=16, fontweight='bold', rotation=90, verticalalignment='center')\n",
    "ax[0].legend(fontsize=9)\n",
    "\n",
    "# DCA\n",
    "TPtrain = np.sum(dfTrain['noECE']==1)/dfTrain.shape[0]\n",
    "FPtrain = np.sum(dfTrain['noECE']==0)/dfTrain.shape[0]\n",
    "for r in models.values():\n",
    "    exactScale = TPtrain/r['DCA_CV']['values'][0]\n",
    "    ax[1].plot(r['DCA_CV']['thresholds'], [x*exactScale for x in r['DCA_CV']['values']], label=r['label'])\n",
    "ax[1].plot(thresh, netBenefitTrain, label='MeasurableECE')\n",
    "# ax[1].plot(thresh, netBenefitCurveCV, label='MeasurableECE CV')\n",
    "\n",
    "# add treat all and treat none\n",
    "ax[1].plot(thresh, (TPtrain - FPtrain*thresh/(1-thresh)), label='Treat all', color='k', linestyle='--')\n",
    "ax[1].plot([0, 1], [0, 0], label='Treat none', color='k', linestyle=':')\n",
    "\n",
    "\n",
    "ax[1].set_title('Net benefit curve', fontsize=16, fontweight='bold')\n",
    "ax[1].legend(fontsize=9)\n",
    "ax[1].set_xlabel('Threshold', fontsize=14)\n",
    "ax[1].set_ylabel('Net benefit curve', fontsize=14)\n",
    "ax[1].set_ylim([-0.05, 0.8])\n",
    "ax[1].set_xlim([0, 0.6])\n",
    "\n",
    "# test data plots\n",
    "\n",
    "# ROC\n",
    "for r in models.values():\n",
    "    fpr, tpr, _ = roc_curve(r['yTest'], r['test_score'])\n",
    "    ax[2].plot(fpr, tpr, label='AUC = ' + str(np.round(r['test_AUC'],3)) + ' ' + r['label'])\n",
    "ax[2].set_xlabel('1 - Specificity', fontsize=14)\n",
    "ax[2].set_ylabel('Sensitivity', fontsize=14)\n",
    "ax[2].text(-0.3, 0.5, 'Test data', fontsize=16, fontweight='bold', rotation=90, verticalalignment='center')\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(dfTest['noECE'], scoreTest)\n",
    "aucTest = roc_auc_score(dfTest['noECE'], scoreTest)\n",
    "ax[2].plot(fpr, tpr, label='AUC = ' + str(np.round(aucTest,3)) + ' MeasurableECE')\n",
    "\n",
    "\n",
    "ax[2].legend(fontsize=9)\n",
    "\n",
    "# DCA\n",
    "for r in models.values():\n",
    "    thr = np.unique(r['test_score'])\n",
    "    thr = np.append(thr, 1e-6)\n",
    "    thr = np.append(thr, 1-1e-6)\n",
    "    thr = np.sort(thr)\n",
    "    tn, fp, fn, tp = confusion_matrix_thresholds(r['yTest'], r['test_score'], thr)\n",
    "    ax[3].plot(thr, (tp - fp * (thr / (1 - thr))) / len(r['yTest']), label=r['label'])\n",
    "ax[3].plot(thresh, netBenefitTest, label='MeasurableECE')\n",
    "ax[3].plot(thresh, (tp[0] - fp[0] * (thresh / (1 - thresh))) / len(r['yTest']), label='Treat all', color='k', linestyle='--')\n",
    "ax[3].plot([0, 1], [0, 0], label='Treat none', color='k', linestyle=':')\n",
    "\n",
    "ax[3].legend(fontsize=9)\n",
    "ax[3].set_xlabel('Threshold', fontsize=14)\n",
    "ax[3].set_ylabel('Net benefit', fontsize=14)\n",
    "ax[3].set_ylim([-0.05, 0.8])\n",
    "ax[3].set_xlim([0, 0.6])\n",
    "\n",
    "\n",
    "figFile = os.path.join('', 'figures', 'ROC_NetBenefit_wide.pdf')\n",
    "if saveFigures:\n",
    "    plt.savefig(figFile)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "ax = ax.ravel()\n",
    "\n",
    "# discovery data plots\n",
    "\n",
    "# FNR\n",
    "for r in models.values():\n",
    "    ax[0].plot(r['FNR_CV']['thresholds'], r['FNR_CV']['values'], label=r['label'])\n",
    "ax[0].set_title('False negative rate', fontsize=16, fontweight='bold')\n",
    "ax[0].legend(fontsize=11)\n",
    "ax[0].set_xlabel('Threshold', fontsize=14)\n",
    "ax[0].set_ylabel('False negative rate', fontsize=14)\n",
    "ax[0].text(-0.3, 0.5, 'Discovery data, cross-validated', fontsize=16, fontweight='bold', rotation=90, verticalalignment='center')\n",
    "\n",
    "# FPR\n",
    "for r in models.values():\n",
    "    ax[1].plot(r['FPR_CV']['thresholds'], r['FPR_CV']['values'], label=r['label'])\n",
    "ax[1].set_title('False positive rate', fontsize=16, fontweight='bold')\n",
    "ax[1].legend(fontsize=11)\n",
    "ax[1].set_xlabel('Threshold', fontsize=14)\n",
    "ax[1].set_ylabel('False positive rate', fontsize=14)\n",
    "\n",
    "\n",
    "# test data plots\n",
    "\n",
    "# FNR, FPR and DCA\n",
    "for r in models.values():\n",
    "    thresh = np.unique(r['test_score'])\n",
    "    thresh = np.append(thresh, 1e-6)\n",
    "    thresh = np.append(thresh, 1-1e-6)\n",
    "    thresh = np.sort(thresh)\n",
    "    tn, fp, fn, tp = confusion_matrix_thresholds(r['yTest'], r['test_score'], thresh)\n",
    "    ax[2].plot(thresh, fn/(fn + tp), label=r['label'])\n",
    "    ax[3].plot(thresh, fp/(fp + tn), label=r['label'])\n",
    "\n",
    "ax[2].text(-0.3, 0.5, 'Test data', fontsize=16, fontweight='bold', rotation=90, verticalalignment='center')\n",
    "\n",
    "for axi in [2, 3]:\n",
    "    ax[axi].legend(fontsize=9)\n",
    "    ax[axi].set_xlabel('Threshold', fontsize=14)\n",
    "\n",
    "ax[2].set_ylabel('False negative rate', fontsize=14)\n",
    "ax[3].set_ylabel('False positive rate', fontsize=14)\n",
    "\n",
    "figFile = os.path.join('', 'figures', 'FP_FN.pdf')\n",
    "if saveFigures:\n",
    "    plt.savefig(figFile)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresh, thresh/(1-thresh),marker='o')\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(thresh,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
