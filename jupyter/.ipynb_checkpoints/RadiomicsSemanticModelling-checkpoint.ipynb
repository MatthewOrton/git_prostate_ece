{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import compress\n",
    "import copy, sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "# from featureSelect_correlation import featureSelect_correlation\n",
    "from pyirr import intraclass_correlation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, RepeatedStratifiedKFold, permutation_test_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve, confusion_matrix\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# use all the processors unless we are in debug mode\n",
    "n_jobs = -1\n",
    "if getattr(sys, 'gettrace', None)():\n",
    "    n_jobs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_two_reader_data(fileName):\n",
    "\n",
    "    # read spreadsheet\n",
    "    df = pd.read_excel(fileName, sheet_name='GG_MG')\n",
    "\n",
    "    # remove features, as with the discovery/test data\n",
    "    df.drop(['IndexLesion_GG', 'IndexLesionMG', 'GlobalStageGG', 'GlobalStageMG'], axis=1, inplace=True)\n",
    "\n",
    "    # remove rows with missing data - need to check that this leaves the same patients for dfGG as in the discovery data set\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # split to each reader\n",
    "    dfGG = df.filter(regex = 'GG|PID', axis = 1)\n",
    "    dfMG = df.filter(regex='MG|PID', axis=1)\n",
    "\n",
    "    # match column names by removing subscripts\n",
    "    dfGG = dfGG.rename(columns=lambda x: x.replace('_GG','').replace('GG',''))\n",
    "    dfMG = dfMG.rename(columns=lambda x: x.replace('_MG','').replace('MG',''))\n",
    "\n",
    "    # change some column names to match the discovery/test data sets\n",
    "    renameDict = {'LocIndexL':'AnatDev01',\n",
    "                  'LocAnat':'AnatDev02',\n",
    "                  'Division':'AnatDev03',\n",
    "                  'DivisionLat':'AnatDev04',\n",
    "                  'LesionSize':'MajorLengthIndex',\n",
    "                  'SmoothCapsularBulgin':'SmoothCapsularBulging',\n",
    "                  'UnsharpMargins':'UnsharpMargin',\n",
    "                  'irregularContour':'IrregularContour',\n",
    "                  'BlackEstrition':'BlackEstritionPeripFat',\n",
    "                  'measurableECE':'MeasurableECE',\n",
    "                  'retroprostaticAngleObl':'RetroprostaticAngleOblit'}\n",
    "    dfGG.rename(renameDict, axis=1, inplace=True)\n",
    "    dfMG.rename(renameDict, axis=1, inplace=True)\n",
    "\n",
    "    # highsignalT1FS is missing from this spreadsheet, so fill in with default value.\n",
    "    # Fortunately, this feature is not selected in the final model, but we need it there for compatibility.\n",
    "    dfGG.loc[:, 'highsignalT1FS'] = 0\n",
    "    dfMG.loc[:, 'highsignalT1FS'] = 0\n",
    "\n",
    "    iccDict = {}\n",
    "    for col in dfGG.drop(['PID', 'highsignalT1FS'], axis=1):\n",
    "        data = np.stack((dfGG[col], dfMG[col]), axis=1)\n",
    "        iccDict[col] = intraclass_correlation(data, \"twoway\", \"agreement\").value\n",
    "\n",
    "    return dfGG, dfMG, iccDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_radiomics_data(radiomicsFile):\n",
    "\n",
    "    df = pd.read_csv(radiomicsFile)\n",
    "    df.drop(list(df.filter(regex = 'source')), axis = 1, inplace = True)\n",
    "    df.drop(list(df.filter(regex = 'diagnostics')), axis = 1, inplace = True)\n",
    "\n",
    "    # split off the repro rows\n",
    "    dfRep1 = df.loc[df.StudyPatientName.str.contains('rep'),:].copy()\n",
    "    dfRep1['StudyPatientName'] = dfRep1['StudyPatientName'].str.replace('_repro','')\n",
    "    dfRep1.sort_values('StudyPatientName', axis=0, inplace=True)\n",
    "    dfRep1.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # remove repro from main data frame\n",
    "    df = df.loc[~df.StudyPatientName.str.contains('rep'),:]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # main data rows for same patients as repro\n",
    "    dfRep0 = df.loc[df['StudyPatientName'].isin(dfRep1['StudyPatientName'])].copy()\n",
    "    dfRep0.sort_values('StudyPatientName', axis=0, inplace=True)\n",
    "    dfRep0.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    iccDict = {}\n",
    "    for col in dfRep1.drop('StudyPatientName', axis=1):\n",
    "        data = np.stack((dfRep0[col], dfRep1[col]), axis=1)\n",
    "        iccDict[col] = intraclass_correlation(data, \"twoway\", \"agreement\").value\n",
    "\n",
    "    return df, iccDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile):\n",
    "    \n",
    "   # load data\n",
    "    dfTrain = pd.read_csv(discoveryFile)\n",
    "    dfTest  = pd.read_csv(externalTestFile)\n",
    "    dfGG, dfMG, iccDictSemantic = load_two_reader_data(twoReaderFile)\n",
    "    dfRad, iccDictRadiomics = load_radiomics_data(radiomicsFile)\n",
    "    \n",
    "    # drop features we are not going to use for classification\n",
    "    dfTrain.drop(['Gleason biopsy','TumorGradeMRI'], inplace=True, axis=1)\n",
    "    dfTest.drop(['Gleason biopsy','TumorGradeMRI'], inplace=True, axis=1)\n",
    "\n",
    "    # merge with clinical features from training data\n",
    "    featuresFromTrainingData = ['PID', 'GleasonBinary', 'ProstateVolume', 'PSA', 'IndLesPIRADS_V2', 'ECE_Pathology']\n",
    "    dfGG = dfGG.merge(dfTrain[featuresFromTrainingData], on='PID')\n",
    "    dfMG = dfMG.merge(dfTrain[featuresFromTrainingData], on='PID')\n",
    "\n",
    "    # make sure columns are ordered the same\n",
    "    dfGG = dfGG[dfTrain.columns]\n",
    "    dfMG = dfMG[dfTrain.columns]\n",
    "\n",
    "    # make these features binary 0/1\n",
    "    toBinary = ['SmoothCapsularBulging' ,'CapsularDisruption', 'UnsharpMargin', 'IrregularContour', 'BlackEstritionPeripFat', 'MeasurableECE', 'RetroprostaticAngleOblit', 'highsignalT1FS']\n",
    "    for tb in toBinary:\n",
    "        dfTrain[tb]  = dfTrain[tb].map(dict(YES=1, NO=0))\n",
    "        dfTest[tb] = dfTest[tb].map(dict(YES=1, NO=0))\n",
    "\n",
    "    # is missing in test and training, so replace both with median from the training data\n",
    "    psaTrainMedian = np.nanmedian(np.array(dfTrain.PSA))\n",
    "    dfTrain.PSA.fillna(psaTrainMedian, inplace=True)\n",
    "    dfTest.PSA.fillna(psaTrainMedian, inplace=True)\n",
    "\n",
    "    # this feature is not selected in the semantic model, so this has no effect\n",
    "    # fill in with the most common value\n",
    "    dfTest.highsignalT1FS.fillna(0, inplace=True)\n",
    "\n",
    "    # extract data into numpy arrays, but keep the feature names\n",
    "    yTrain = np.array(dfTrain.ECE_Pathology)\n",
    "    yTest = np.array(dfTest.ECE_Pathology)\n",
    "    yMG_GG = np.array(dfGG.ECE_Pathology)\n",
    "\n",
    "    XTrain = dfTrain.drop(['PID', 'ECE_Pathology'], axis=1)\n",
    "    XTest = dfTest.drop(['PID', 'ECE_Pathology'], axis=1)\n",
    "    X_GG = dfGG.drop(['PID', 'ECE_Pathology'], axis=1)\n",
    "    X_MG = dfMG.drop(['PID', 'ECE_Pathology'], axis=1)\n",
    "\n",
    "    featureNames = list(XTrain.columns)\n",
    "    XTrain = np.array(XTrain)\n",
    "    XTest = np.array(XTest)\n",
    "    X_GG = np.array(X_GG)\n",
    "    X_MG = np.array(X_MG)\n",
    "\n",
    "    return XTrain, yTrain, XTest, yTest, X_GG, X_MG, yMG_GG, featureNames, iccDictSemantic, iccDictRadiomics, dfRad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_training_data(XTrain, yTrain, featureNames, iccDict, n_repeats=10, n_splits=10, n_permutations=10, crossValidateFit=True, printResubstitutionMetrics=False):\n",
    "\n",
    "    # Check for data leak - this is in case the way this code is split into various functions\n",
    "    # accidentally allows scope of the test data to include function.\n",
    "    if 'yTest' in locals() or 'yTest' in globals():\n",
    "        print('Test data is accessible to training function - check code for data leak!!!')\n",
    "        return {}, None\n",
    "\n",
    "    # reproducible execution\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # logistic LASSO tuning parameter optimised using function with in-built CV\n",
    "    pipeline = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                               ('logistic',LogisticRegressionCV(Cs=20, \n",
    "                                                                cv=10, \n",
    "                                                                solver=\"liblinear\",\n",
    "                                                                max_iter=10000, penalty='l1',\n",
    "                                                                random_state=seed))])\n",
    "\n",
    "    # fit to all data\n",
    "    pipeline.fit(XTrain, yTrain)\n",
    "\n",
    "    # print some performance metrics\n",
    "    if printResubstitutionMetrics:\n",
    "        y_pred_score = pipeline.predict_proba(XTrain)[:, 1]\n",
    "        y_pred_class = pipeline.predict(XTrain)\n",
    "        resubAUROC = roc_auc_score(yTrain, y_pred_score)\n",
    "        resubAccuracy = accuracy_score(yTrain, y_pred_class)\n",
    "        resubF1 = f1_score(yTrain, y_pred_class)\n",
    "\n",
    "        print('AUCROC  (resub) = ' + str(np.round(resubAUROC, 3)))\n",
    "        print('Accuracy (resub) = ' + str(np.round(resubAccuracy, 3)))\n",
    "        print('F1 (resub) = ' + str(np.round(resubF1, 3)))\n",
    "        print(' ')\n",
    "\n",
    "    # default value for this when crossValidateFit = False\n",
    "    dfCoefResults = None\n",
    "    \n",
    "    if crossValidateFit:\n",
    "\n",
    "        # cross-validate\n",
    "        outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "        cv_result = cross_validate(pipeline, X=XTrain, y=yTrain, cv=outer_cv, scoring=['accuracy', 'roc_auc', 'f1'],\n",
    "                                   return_estimator=True, verbose=0, n_jobs=n_jobs)\n",
    "\n",
    "        # get frequency that features are non-zero across the repeated cv splits\n",
    "        coef_cv = np.zeros((len(cv_result['estimator']), XTrain.shape[1]))\n",
    "        for n, res in enumerate(cv_result['estimator']):\n",
    "            coef_cv[n, :] = res._final_estimator.coef_\n",
    "        coef_freq = np.sum(coef_cv != 0, axis=0) / (n_repeats * n_splits)\n",
    "\n",
    "        # put icc values in array for including in DataFrame\n",
    "        iccList = []\n",
    "        for feat in featureNames:\n",
    "            if feat in iccDict:\n",
    "                iccList.append(iccDict[feat])\n",
    "            else:\n",
    "                iccList.append('-')\n",
    "\n",
    "        # display sorted coefficients and selection frequency\n",
    "        coeffs = np.squeeze(pipeline._final_estimator.coef_)\n",
    "        dfCoefResults = pd.DataFrame({'Feature': featureNames, 'Coeff': coeffs, 'Freq': coef_freq, 'ICC':iccList})\n",
    "        dfCoefResults.sort_values(by=['Coeff', 'Freq'], key=abs, inplace=True, ascending=False)\n",
    "\n",
    "        # print CV scores\n",
    "        print('AUCROC   (CV) = ' + str(np.mean(cv_result['test_roc_auc']).round(3)))\n",
    "        print('Accuracy (CV) = ' + str(np.mean(cv_result['test_accuracy']).round(3)))\n",
    "        print('F1       (CV) = ' + str(np.mean(cv_result['test_f1']).round(3)))\n",
    "\n",
    "        # permutation testing\n",
    "        outer_cv.n_repeats = 1\n",
    "        scoreDirect, perm_scores, pValueDirect = permutation_test_score(pipeline, XTrain, yTrain, scoring=\"roc_auc\",\n",
    "                                                                        cv=outer_cv, n_permutations=n_permutations,\n",
    "                                                                        verbose=0, n_jobs=n_jobs)\n",
    "\n",
    "        # pValueDirect is computed using scoreDirect and assumes only one outer CV run\n",
    "        # We have used repeated outer CV, so the following code correctly computes the p-value of our repeated CV performance estimate\n",
    "        # Actually, it doesn't seem to make much difference, so am relaxed about that.\n",
    "\n",
    "        p_values = []\n",
    "        scores_roc_auc = np.mean(np.reshape(cv_result['test_roc_auc'], (n_repeats, -1)), axis=1)\n",
    "        for score in scores_roc_auc:\n",
    "            p_values.append((np.count_nonzero(perm_scores >= score) + 1) / (n_permutations + 1))\n",
    "        print('p-value       = ' + str(np.mean(p_values).round(4)) + '\\n\\n')\n",
    "\n",
    "    return pipeline, dfCoefResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fit_training_test(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile, n_splits=10, n_repeats=10, n_permutations=10):\n",
    "\n",
    "    # get all required data arrays\n",
    "    XTrain, yTrain, XTest, yTest, X_GG, X_MG, yMG_GG, featureNames, iccDict = load_data(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile)\n",
    "\n",
    "    # fit training data using cross-validation and permutation testing\n",
    "    pipeline, dfCoefResults = fit_training_data(XTrain, yTrain, featureNames, iccDict, \n",
    "                               n_splits=n_splits, n_repeats=n_repeats, \n",
    "                               n_permutations=n_permutations, crossValidateFit=True)\n",
    "\n",
    "    # re-fit training data using features selected based on the frequency \n",
    "    # the logisticLASSO retains each feature being > 0.9\n",
    "    selectedFeatures = ['GleasonBinary', 'MeasurableECE', 'CapsularContactLength', \n",
    "                        'IrregularContour', 'CapsularDisruption']\n",
    "    XTrain_reducedModel = np.array(pd.DataFrame(XTrain, columns=featureNames)[selectedFeatures])\n",
    "    XTest_reducedModel = np.array(pd.DataFrame(XTest, columns=featureNames)[selectedFeatures])\n",
    "    #\n",
    "    pipeline_reducedModel, _ = fit_training_data(XTrain_reducedModel, yTrain, featureNames, iccDict, crossValidateFit=False)\n",
    "\n",
    "    # package lots of variables up into a dict for tidy output\n",
    "    outputVars = ('pipeline', 'dfCoefResults', 'pipeline_reducedModel', 'XTrain', 'XTrain_reducedModel', \n",
    "                  'yTrain', 'XTest', 'XTest_reducedModel', 'yTest', 'X_GG', 'X_MG', 'yMG_GG')\n",
    "    out = {}\n",
    "    for var in outputVars:\n",
    "        out[var] = locals()[var]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. , 44. ,  8.9, ...,  0. ,  0. ,  0. ],\n",
       "       [ 0. , 27. ,  5.9, ...,  0. ,  0. ,  0. ],\n",
       "       [ 1. , 42. ,  3.6, ...,  1. ,  1. ,  0. ],\n",
       "       ...,\n",
       "       [ 0. , 58. ,  7.5, ...,  0. ,  0. ,  0. ],\n",
       "       [ 0. , 35. ,  3.4, ...,  0. ,  0. ,  0. ],\n",
       "       [ 1. , 42. ,  5.3, ...,  1. ,  0. ,  0. ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m display(XTrain)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# unpack dictionary to variables\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mlocals\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mresult\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# Load data and fit models\n",
    "\n",
    "discoveryFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/discovery.csv')\n",
    "externalTestFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/external.csv')\n",
    "twoReaderFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/GG_MG.xlsx')\n",
    "radiomicsFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/radiomicFeatures.csv')\n",
    "\n",
    "n_splits=10\n",
    "n_repeats=100\n",
    "n_permutations=1000\n",
    "\n",
    "# result = load_fit_training_test(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile,\n",
    "#                                 n_splits=n_splits, n_repeats=n_repeats, n_permutations=n_permutations)\n",
    "\n",
    "XTrain, yTrain, XTest, yTest, X_GG, X_MG, yMG_GG, featureNames, iccDictSemantic, iccDictRadiomics, dfRad = load_data(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile)\n",
    "\n",
    "# unpack dictionary to variables\n",
    "locals().update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision', 3)\n",
    "pd.set_option('display.colheader_justify','left')\n",
    "display(dfCoefResults.style.hide_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores and predicted class info\n",
    "y_pred_score_test = pipeline.predict_proba(XTest)[:, 1]\n",
    "y_pred_class_test = pipeline.predict(XTest)\n",
    "y_pred_score_test_reducedModel = pipeline_reducedModel.predict_proba(XTest_reducedModel)[:, 1]\n",
    "y_pred_class_test_reducedModel = pipeline_reducedModel.predict(XTest_reducedModel)\n",
    "y_pred_score_GG = pipeline.predict_proba(X_GG)[:, 1]\n",
    "y_pred_score_MG = pipeline.predict_proba(X_MG)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scores from main model\n",
    "testAUROC = roc_auc_score(yTest, y_pred_score_test)\n",
    "testAccuracy = accuracy_score(yTest, y_pred_class_test)\n",
    "testF1 = f1_score(yTest, y_pred_class_test)\n",
    "\n",
    "# test scores from reduced model\n",
    "test_reducedModel_AUROC = roc_auc_score(yTest, y_pred_score_test_reducedModel)\n",
    "test_reducedModel_Accuracy = accuracy_score(yTest, y_pred_class_test_reducedModel)\n",
    "test_reducedModel_F1 = f1_score(yTest, y_pred_class_test_reducedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the test performance metrics\n",
    "print('Principle model')\n",
    "print('AUCROC  (test)  = ' + str(np.round(testAUROC,3)))\n",
    "print('Accuracy (test) = ' + str(np.round(testAccuracy,3)))\n",
    "print('F1 (test)       = ' + str(np.round(testF1,3)))\n",
    "\n",
    "print('\\nReduced model using GleasonBinary, MeasurableECE, CapsularContactLength, IrregularContour, CapsularDisruption')\n",
    "print('AUCROC   = ' + str(np.round(test_reducedModel_AUROC,3)))\n",
    "print('Accuracy = ' + str(np.round(test_reducedModel_Accuracy,3)))\n",
    "print('F1       = ' + str(np.round(test_reducedModel_F1,3)))\n",
    "print(' ')\n",
    "\n",
    "data = np.stack((y_pred_score_MG, y_pred_score_GG), axis=1)\n",
    "iccScore = intraclass_correlation(data, \"twoway\", \"agreement\").value\n",
    "print('ICC comparing GG and MG scores  = ' + str(np.round(iccScore,3)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_thresholds(yTrue, yScore, thresholds):\n",
    "    tnArr, fpArr, fnArr, tpArr = [], [], [], []\n",
    "    nSamples = len(yTrue)\n",
    "    for thresh in thresholds:\n",
    "        tn, fp, fn, tp = confusion_matrix(yTrue, yScore>thresh).ravel()\n",
    "        tnArr.append(tn/nSamples)\n",
    "        fpArr.append(fp/nSamples)\n",
    "        fnArr.append(fn/nSamples)\n",
    "        tpArr.append(tp/nSamples)\n",
    "    return np.array(tnArr), np.array(fpArr), np.array(fnArr), np.array(tpArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot comparing scores\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.scatter(y_pred_score_GG, y_pred_score_MG, c=yMG_GG, s=10, cmap='bwr')\n",
    "plt.xlabel('Reader 1 score')\n",
    "plt.ylabel('Reader 2 score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot comparing ROCs\n",
    "fprGG, tprGG, _ = roc_curve(yMG_GG, y_pred_score_GG)\n",
    "fprMG, tprMG, _ = roc_curve(yMG_GG, y_pred_score_MG)\n",
    "fprTest, tprTest, _ = roc_curve(yTest, y_pred_score_test)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.plot(fprGG, tprGG,     label='Train, reader 1, AUROC = ' + str(np.round(roc_auc_score(yMG_GG, y_pred_score_GG),3)))\n",
    "plt.plot(fprMG, tprMG,     label='Train, reader 2, AUROC = ' + str(np.round(roc_auc_score(yMG_GG, y_pred_score_MG),3)))\n",
    "plt.plot(fprTest, tprTest, label='Test,  reader 1, AUROC = ' + str(np.round(roc_auc_score(yTest, y_pred_score_test),3)))\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.title('ROCs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots comparing TP, FP, Sensitivity etc.\n",
    "\n",
    "thresh = np.linspace(0, 1, 500)\n",
    "tnTest, fpTest, fnTest, tpTest = roc_curve_thresholds(yTest, y_pred_score_test, thresh)\n",
    "tnGG, fpGG, fnGG, tpGG = roc_curve_thresholds(yMG_GG, y_pred_score_GG, thresh)\n",
    "tnMG, fpMG, fnMG, tpMG = roc_curve_thresholds(yMG_GG, y_pred_score_MG, thresh)\n",
    "\n",
    "_, ax = plt.subplots(2,3, figsize=(16,12))\n",
    "ax = ax.ravel()\n",
    "\n",
    "ax[0].plot(thresh, tpGG/(tpGG + fnGG), label='Train, reader 1')\n",
    "ax[0].plot(thresh, tpMG/(tpMG + fnMG), label='Train, reader 2')\n",
    "ax[0].plot(thresh, tpTest/(tpTest + fnTest), label='Test,  reader 1')\n",
    "ax[0].set_title('True positive rate (Sensitivity)')\n",
    "ax[0].set_xlabel('Threshold')\n",
    "ax[0].set_ylabel('TPR')\n",
    "\n",
    "ax[1].plot(thresh, tnGG/(tnGG + fpGG), label='Train, reader 1')\n",
    "ax[1].plot(thresh, tnMG/(tnMG + fpMG), label='Train, reader 2')\n",
    "ax[1].plot(thresh, tnTest/(tnTest + fpTest), label='Test,  reader 1')\n",
    "ax[1].set_title('True negative rate (Specificity)')\n",
    "ax[1].set_xlabel('Threshold')\n",
    "ax[1].set_ylabel('TNR')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(thresh, fnGG/(fnGG + tpGG))\n",
    "ax[2].plot(thresh, fnMG/(fnMG + tpMG))\n",
    "ax[2].plot(thresh, fnTest/(fnTest + tpTest))\n",
    "ax[2].set_title('False negative rate')\n",
    "ax[2].set_xlabel('Threshold')\n",
    "ax[2].set_ylabel('FNR')\n",
    "\n",
    "ax[3].plot(thresh, fpGG/(fpGG + tnGG))\n",
    "ax[3].plot(thresh, fpMG/(fpMG + tnMG))\n",
    "ax[3].plot(thresh, fpTest/(fpTest + tnTest))\n",
    "ax[3].set_title('False positive rate')\n",
    "ax[3].set_xlabel('Threshold')\n",
    "ax[3].set_ylabel('FPR')\n",
    "\n",
    "ax[4].plot(thresh, (tnGG + tpGG)/(tpGG + fnGG + tnGG + fpGG))\n",
    "ax[4].plot(thresh, (tnMG + tpMG)/(tpMG + fnMG + tnMG + fpMG))\n",
    "ax[4].plot(thresh, (tnTest + tpTest)/(tpTest + fnTest + tnTest + fpTest))\n",
    "ax[4].set_title('Accuracy')\n",
    "ax[4].set_xlabel('Threshold')\n",
    "ax[4].set_ylabel('Accuracy')\n",
    "\n",
    "ax[5].plot(thresh, 2*tpGG/(2*tpGG + fnGG + fpGG))\n",
    "ax[5].plot(thresh, 2*tpMG/(2*tpMG + fnMG + fpMG))\n",
    "ax[5].plot(thresh, 2*tpTest/(2*tpTest + fnTest + fpTest))\n",
    "ax[5].set_title('F1')\n",
    "ax[5].set_xlabel('Threshold')\n",
    "ax[5].set_ylabel('F1')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].plot(thresh, tpGG/(tpGG + fnGG), label='Train, reader 1')\n",
    "ax[0].plot(thresh, tpMG/(tpMG + fnMG), label='Train, reader 2')\n",
    "ax[0].plot(thresh, tpTest/(tpTest + fnTest), label='Test,  reader 1')\n",
    "ax[0].set_title('True positive rate (Sensitivity)')\n",
    "ax[0].set_xlim([0, 0.5])\n",
    "ax[0].set_ylim([0.5, 1.05])\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Threshold')\n",
    "ax[0].set_ylabel('TPR')\n",
    "\n",
    "ax[1].plot(thresh, fnGG/(fnGG + tpGG), label='Train, reader 1')\n",
    "ax[1].plot(thresh, fnMG/(fnMG + tpMG), label='Train, reader 2')\n",
    "ax[1].plot(thresh, fnTest/(fnTest + tpTest), label='Test,  reader 1')\n",
    "ax[1].set_title('False negative rate')\n",
    "ax[1].set_xlim([-0.05, 0.5])\n",
    "ax[1].set_ylim([-0.05, 0.5])\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Threshold')\n",
    "ax[1].set_ylabel('FNR')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].plot(thresh, tpGG, label='Train, reader 1')\n",
    "ax[0].plot(thresh, tpMG, label='Train, reader 2')\n",
    "ax[0].plot(thresh, tpTest, label='Test,  reader 1')\n",
    "ax[0].set_xlim([0, 0.5])\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Threshold')\n",
    "ax[0].set_ylabel('TP')\n",
    "ax[0].set_title('True positives')\n",
    "\n",
    "ax[1].plot(thresh, fnGG, label='Train, reader 1')\n",
    "ax[1].plot(thresh, fnMG, label='Train, reader 2')\n",
    "ax[1].plot(thresh, fnTest, label='Test,  reader 1')\n",
    "ax[1].set_title('False negatives')\n",
    "ax[1].set_xlim([0, 0.5])\n",
    "#ax[1].set_ylim([0, 0.5])\n",
    "ax[1].legend()\n",
    "ax[0].set_xlabel('Threshold')\n",
    "ax[1].set_ylabel('FN')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
