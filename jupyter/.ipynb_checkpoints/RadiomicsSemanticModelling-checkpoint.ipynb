{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('precision', 3)\n",
    "from itertools import compress\n",
    "import copy, sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "from pyirr import intraclass_correlation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, RepeatedStratifiedKFold, permutation_test_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve, confusion_matrix, make_scorer\n",
    "from scipy.stats import spearmanr, mannwhitneyu\n",
    "\n",
    "oldPath = os.path.join(os.path.expanduser('~'), 'Documents/GitHub/icrpythonradiomics/machineLearning')\n",
    "if os.path.exists(oldPath):\n",
    "    sys.path.remove(oldPath)\n",
    "sys.path.append(os.path.join(os.path.expanduser('~'), 'Documents/git/git_icrpythonradiomics/machineLearning'))\n",
    "from featureSelection import featureSelection_correlation\n",
    "\n",
    "# validation parameters\n",
    "n_splits = 10\n",
    "n_repeats = 100\n",
    "n_permutations = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def roc_curve_thresholds(yTrue, yScore, thresholds):\n",
    "    tnArr, fpArr, fnArr, tpArr = [], [], [], []\n",
    "    nSamples = len(yTrue)\n",
    "    for thresh in thresholds:\n",
    "        tn, fp, fn, tp = confusion_matrix(yTrue, yScore>thresh).ravel()\n",
    "        tnArr.append(tn/nSamples)\n",
    "        fpArr.append(fp/nSamples)\n",
    "        fnArr.append(fn/nSamples)\n",
    "        tpArr.append(tp/nSamples)\n",
    "    return np.array(tnArr), np.array(fpArr), np.array(fnArr), np.array(tpArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_fnr(y_true, y_pred, pt=0):\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred > pt).ravel()\n",
    "    return fn/(fn + tp)\n",
    "\n",
    "def calculate_fpr(y_true, y_pred, pt=0):\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred > pt).ravel()\n",
    "    return fp/(fp + tn)\n",
    "\n",
    "def unpack_scorers(cvr):\n",
    "\n",
    "    thresh_FNR, thresh_FPR, value_FNR, value_FPR = [], [], [], []\n",
    "    \n",
    "    for key, value in cvr.items():\n",
    "        if 'test_FNR' in key:\n",
    "            thresh_FNR.append(float(key.replace('test_FNR_','')))\n",
    "            value_FNR.append(np.mean(value))\n",
    "        if 'test_FPR' in key:\n",
    "            thresh_FPR.append(float(key.replace('test_FPR_','')))\n",
    "            value_FPR.append(np.mean(value))\n",
    "\n",
    "    idxFNR = np.argsort(thresh_FNR)\n",
    "    idxFPR = np.argsort(thresh_FPR)\n",
    "\n",
    "    FNR = {'thresholds': [thresh_FNR[idx] for idx in idxFNR],\n",
    "           'values': [value_FNR[idx] for idx in idxFNR]}\n",
    "    \n",
    "    FPR = {'thresholds': [thresh_FPR[idx] for idx in idxFPR],\n",
    "           'values': [value_FPR[idx] for idx in idxFPR]}\n",
    "    \n",
    "    return FNR, FPR\n",
    "\n",
    "# Make dictionary of scorers, each of which will compute one point on the FNR and FPR curves.\n",
    "# The dictionary key is used to keep track of the threshold value that was used.\n",
    "scorers = {}\n",
    "\n",
    "# Don't use 0 and 1 as endpoints as this causes numerical underflow.\n",
    "ptArr = np.round(np.linspace(0, 1, 501),2)\n",
    "ptArr[0] = np.round(0.0001,4)\n",
    "ptArr[-1] = np.round(0.9999,4)\n",
    "\n",
    "for pt in ptArr:\n",
    "    scorers['FNR_' + str(pt)] = make_scorer(calculate_fnr, pt = pt, needs_proba=True)\n",
    "    scorers['FPR_' + str(pt)] = make_scorer(calculate_fpr, pt = pt, needs_proba=True)\n",
    "\n",
    "# standard scorers    \n",
    "scorers['roc_auc'] = make_scorer(roc_auc_score)\n",
    "scorers['accuracy'] = make_scorer(accuracy_score)\n",
    "scorers['f1'] = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load two reader data - for ICC\n",
    "\n",
    "twoReaderFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/GG_MG.xlsx')\n",
    "\n",
    "# read spreadsheet\n",
    "df = pd.read_excel(twoReaderFile, sheet_name='GG_MG', engine='openpyxl')\n",
    "\n",
    "# remove features, as with the discovery/test data\n",
    "df.drop(['IndexLesion_GG', 'IndexLesionMG', 'GlobalStageGG', 'GlobalStageMG'], axis=1, inplace=True)\n",
    "\n",
    "# remove rows with missing data - need to check that this leaves the same patients for dfGG as in the discovery data set\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# split to each reader\n",
    "dfGG = df.filter(regex = 'GG|PID', axis = 1)\n",
    "dfMG = df.filter(regex='MG|PID', axis=1)\n",
    "\n",
    "# match column names by removing subscripts\n",
    "dfGG = dfGG.rename(columns=lambda x: x.replace('_GG','').replace('GG',''))\n",
    "dfMG = dfMG.rename(columns=lambda x: x.replace('_MG','').replace('MG',''))\n",
    "\n",
    "# change some column names to match the discovery/test data sets\n",
    "renameDict = {'LocIndexL':'AnatDev01',\n",
    "              'LocAnat':'AnatDev02',\n",
    "              'Division':'AnatDev03',\n",
    "              'DivisionLat':'AnatDev04',\n",
    "              'LesionSize':'MajorLengthIndex',\n",
    "              'SmoothCapsularBulgin':'SmoothCapsularBulging',\n",
    "              'UnsharpMargins':'UnsharpMargin',\n",
    "              'irregularContour':'IrregularContour',\n",
    "              'BlackEstrition':'BlackEstritionPeripFat',\n",
    "              'measurableECE':'MeasurableECE',\n",
    "              'retroprostaticAngleObl':'RetroprostaticAngleOblit'}\n",
    "dfGG.rename(renameDict, axis=1, inplace=True)\n",
    "dfMG.rename(renameDict, axis=1, inplace=True)\n",
    "\n",
    "# highsignalT1FS is missing from this spreadsheet, so fill in with default value.\n",
    "# Fortunately, this feature is not selected in the final model, but we need it there for compatibility.\n",
    "dfGG.loc[:, 'highsignalT1FS'] = 0\n",
    "dfMG.loc[:, 'highsignalT1FS'] = 0\n",
    "\n",
    "iccDict = {}\n",
    "for col in dfGG.drop(['PID', 'highsignalT1FS'], axis=1):\n",
    "    data = np.stack((dfGG[col], dfMG[col]), axis=1)\n",
    "    iccDict[col] = intraclass_correlation(data, \"twoway\", \"agreement\").value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radiomicsFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/radiomicFeatures.csv')\n",
    "\n",
    "dfRad = pd.read_csv(radiomicsFile)\n",
    "dfRad.drop(list(dfRad.filter(regex = 'source')), axis = 1, inplace = True)\n",
    "dfRad.drop(list(dfRad.filter(regex = 'diagnostics')), axis = 1, inplace = True)\n",
    "dfRad.drop(list(dfRad.filter(regex = 'histogram')), axis = 1, inplace = True)\n",
    "\n",
    "# To match the semantic data file\n",
    "dfRad['StudyPatientName'] = dfRad['StudyPatientName'].str.replace('_',' ')\n",
    "\n",
    "# sensible prefix \n",
    "dfRad = dfRad.rename(columns=lambda x: x.replace('original','radiomics'))\n",
    "\n",
    "# split off the repro rows\n",
    "dfRep1 = dfRad.loc[dfRad.StudyPatientName.str.contains('rep'),:].copy()\n",
    "dfRep1['StudyPatientName'] = dfRep1['StudyPatientName'].str.replace(' repro','')\n",
    "dfRep1.sort_values('StudyPatientName', axis=0, inplace=True)\n",
    "dfRep1.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# remove repro from main data frame\n",
    "dfRad = dfRad.loc[~dfRad.StudyPatientName.str.contains('rep'),:]\n",
    "dfRad.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# main data rows for same patients as repro\n",
    "dfRep0 = dfRad.loc[dfRad['StudyPatientName'].isin(dfRep1['StudyPatientName'])].copy()\n",
    "dfRep0.sort_values('StudyPatientName', axis=0, inplace=True)\n",
    "dfRep0.reset_index(inplace=True, drop=True)\n",
    "\n",
    "for col in dfRep1.drop('StudyPatientName', axis=1):\n",
    "    data = np.stack((dfRep0[col], dfRep1[col]), axis=1)\n",
    "    iccDict[col] = intraclass_correlation(data, \"twoway\", \"agreement\").value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discoveryFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/discovery.csv')\n",
    "externalTestFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/external.csv')\n",
    "    \n",
    "# load data\n",
    "dfTrain = pd.read_csv(discoveryFile)\n",
    "dfTest  = pd.read_csv(externalTestFile)\n",
    "\n",
    "# drop features we are not going to use for classification\n",
    "dfTrain.drop(['Gleason biopsy','TumorGradeMRI'], inplace=True, axis=1)\n",
    "dfTest.drop(['Gleason biopsy','TumorGradeMRI'], inplace=True, axis=1)\n",
    "\n",
    "# make these features binary 0/1\n",
    "toBinary = ['SmoothCapsularBulging' ,'CapsularDisruption', 'UnsharpMargin', 'IrregularContour', 'BlackEstritionPeripFat', 'MeasurableECE', 'RetroprostaticAngleOblit', 'highsignalT1FS']\n",
    "for tb in toBinary:\n",
    "    dfTrain[tb]  = dfTrain[tb].map(dict(YES=1, NO=0))\n",
    "    dfTest[tb] = dfTest[tb].map(dict(YES=1, NO=0))\n",
    "\n",
    "# is missing in test and training, so replace both with median from the training data\n",
    "psaTrainMedian = np.nanmedian(np.array(dfTrain.PSA))\n",
    "dfTrain.PSA.fillna(psaTrainMedian, inplace=True)\n",
    "dfTest.PSA.fillna(psaTrainMedian, inplace=True)\n",
    "\n",
    "# this feature is not selected in the semantic model, so this has no effect\n",
    "# fill in with the most common value\n",
    "dfTest.highsignalT1FS.fillna(0, inplace=True)\n",
    "\n",
    "# add string to names for easy manipulation of groups\n",
    "clinicalFeatures = ['GleasonBinary', 'ProstateVolume', 'PSA', 'IndLesPIRADS_V2']\n",
    "semanticFeatures = list(set(dfTrain.columns) - set(clinicalFeatures) - set(['PID', 'ECE_Pathology']))\n",
    "dfTrain = dfTrain.rename(columns=lambda x: 'clinical_' + x if x in clinicalFeatures else x)\n",
    "dfTest = dfTest.rename(columns=lambda x: 'clinical_' + x if x in clinicalFeatures else x)\n",
    "dfTrain = dfTrain.rename(columns=lambda x: 'semantic_' + x if x in semanticFeatures else x)\n",
    "dfTest = dfTest.rename(columns=lambda x: 'semantic_' + x if x in semanticFeatures else x)\n",
    "\n",
    "# merge radiomics \n",
    "dfTrain = dfTrain.merge(dfRad, left_on='PID', right_on='StudyPatientName')\n",
    "dfTest = dfTest.merge(dfRad, left_on='PID', right_on='StudyPatientName')\n",
    "dfTrain = dfTrain.drop(['PID', 'StudyPatientName'], axis=1)\n",
    "dfTest = dfTest.drop(['PID', 'StudyPatientName'], axis=1)\n",
    "\n",
    "# prediction target\n",
    "yTrain = np.array(dfTrain.ECE_Pathology)\n",
    "yTest = np.array(dfTest.ECE_Pathology)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model using only clinical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fitClinical(displayCoef=False):\n",
    "    \n",
    "    print('\\nBaseline model using only clinical features\\n')\n",
    "\n",
    "    # reproducible execution\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # get training and test data\n",
    "    XTrain = dfTrain.drop('ECE_Pathology', axis=1)\n",
    "    XTest = dfTest.drop('ECE_Pathology', axis=1)\n",
    "\n",
    "    XTrain.drop(list(XTrain.filter(regex = 'semantic|radiomic')), axis = 1, inplace = True)\n",
    "    XTest.drop(list(XTest.filter(regex = 'semantic|radiomic')), axis = 1, inplace = True)\n",
    "\n",
    "    # logistic LASSO tuning parameter optimised using function with in-built CV\n",
    "    pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                                       ('lr', LogisticRegression(penalty='none'))])\n",
    "\n",
    "    # fit to all data\n",
    "    pipeline.fit(XTrain, yTrain)\n",
    "\n",
    "    # cross-validate\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "    cv_result = cross_validate(pipeline, X=XTrain, y=yTrain, cv=outer_cv, scoring=scorers,\n",
    "                               return_estimator=True, verbose=0, n_jobs=-1)\n",
    "\n",
    "    FNR, FPR = unpack_scorers(cv_result)\n",
    "\n",
    "    # print CV scores\n",
    "    AUC_CV = np.mean(cv_result['test_roc_auc'])\n",
    "    print('AUCROC   (CV) = ' + str(AUC_CV.round(3)))\n",
    "    print('Accuracy (CV)  = ' + str(np.mean(cv_result['test_accuracy']).round(3)))\n",
    "    print('F1       (CV)  = ' + str(np.mean(cv_result['test_f1']).round(3)))\n",
    "\n",
    "    # permutation testing\n",
    "    outer_cv.n_repeats = 1\n",
    "    scoreDirect, perm_scores, pValueDirect = permutation_test_score(pipeline, XTrain, yTrain, scoring=\"roc_auc\",\n",
    "                                                                    cv=outer_cv, n_permutations=n_permutations,\n",
    "                                                                    verbose=0, n_jobs=-1)\n",
    "\n",
    "    # pValueDirect is computed using scoreDirect and assumes only one outer CV run\n",
    "    # We have used repeated outer CV, so the following code correctly computes the p-value of our repeated CV performance estimate\n",
    "    # Actually, it doesn't seem to make much difference, so am relaxed about that.\n",
    "\n",
    "    p_values = []\n",
    "    scores_roc_auc = np.mean(np.reshape(cv_result['test_roc_auc'], (n_repeats, -1)), axis=1)\n",
    "    for score in scores_roc_auc:\n",
    "        p_values.append((np.count_nonzero(perm_scores >= score) + 1) / (n_permutations + 1))\n",
    "    print('p-value  (CV) = ' + str(np.mean(p_values).round(4)) + '\\n')\n",
    "\n",
    "    # get scores and predicted class info\n",
    "    y_pred_score_test = pipeline.predict_proba(XTest)[:, 1]\n",
    "    y_pred_class_test = pipeline.predict(XTest)\n",
    "\n",
    "    # test scores from main model\n",
    "    testAUROC = roc_auc_score(yTest, y_pred_score_test)\n",
    "    testAccuracy = accuracy_score(yTest, y_pred_class_test)\n",
    "    testF1 = f1_score(yTest, y_pred_class_test)\n",
    "\n",
    "    pValueTest = mannwhitneyu(y_pred_score_test[yTest==0], y_pred_score_test[yTest==1]).pvalue\n",
    "\n",
    "    # print the test performance metrics\n",
    "    print('AUCROC   (test) = ' + str(np.round(testAUROC,3)))\n",
    "    print('Accuracy (test) = ' + str(np.round(testAccuracy,3)))\n",
    "    print('F1       (test) = ' + str(np.round(testF1,3)))\n",
    "    print('p-value  (test) = ' + str(np.round(pValueTest, 6)) + '\\n')\n",
    "\n",
    "    # show coefficients\n",
    "    if displayCoef:\n",
    "        display(pd.DataFrame({'Feature':[x.replace('clinical_','') for x in XTrain.columns], 'Coefficient':np.squeeze(pipelineClinical._final_estimator.coef_)}).style.hide_index())\n",
    "\n",
    "    return AUC_CV, y_pred_score_test, y_pred_class_test, FNR, FPR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical + Semantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fitClinicalSemantic(displayCoef=False, permutationTest=False):\n",
    "\n",
    "    print('\\n\\n\\nModel using clinical and semantic features\\n')\n",
    "\n",
    "    # reproducible execution\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # get training and test data\n",
    "    XTrain = dfTrain.drop('ECE_Pathology', axis=1)\n",
    "    XTest = dfTest.drop('ECE_Pathology', axis=1)\n",
    "\n",
    "    XTrain.drop(list(XTrain.filter(regex = 'radiomic')), axis = 1, inplace = True)\n",
    "    XTest.drop(list(XTest.filter(regex = 'radiomic')), axis = 1, inplace = True)\n",
    "\n",
    "    # logistic LASSO tuning parameter optimised using function with in-built CV\n",
    "    pipeline = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                                       ('logistic',LogisticRegressionCV(Cs=20, \n",
    "                                                                        cv=10, \n",
    "                                                                        solver=\"liblinear\",\n",
    "                                                                        max_iter=10000, penalty='l1',\n",
    "                                                                        random_state=seed,\n",
    "                                                                        n_jobs=1))])\n",
    "\n",
    "    # fit to all data\n",
    "    pipeline.fit(XTrain, yTrain)\n",
    "\n",
    "\n",
    "    # cross-validate\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "    cv_result = cross_validate(pipeline, X=XTrain, y=yTrain, cv=outer_cv, scoring=scorers,\n",
    "                               return_estimator=True, verbose=0, n_jobs=-1)\n",
    "\n",
    "    FNR, FPR = unpack_scorers(cv_result)\n",
    "\n",
    "    # get frequency that features are non-zero across the repeated cv splits\n",
    "    coef_cv = np.zeros((len(cv_result['estimator']), XTrain.shape[1]))\n",
    "    for n, res in enumerate(cv_result['estimator']):\n",
    "        coef_cv[n, :] = res._final_estimator.coef_\n",
    "    coef_freq = np.sum(coef_cv != 0, axis=0) / (n_repeats * n_splits)\n",
    "\n",
    "    # put icc values in array for including in DataFrame\n",
    "    iccList = []\n",
    "    for feat in XTrain.columns:\n",
    "        if feat in iccDict:\n",
    "            iccList.append(iccDict[feat])\n",
    "        else:\n",
    "            iccList.append('-')\n",
    "\n",
    "    # display sorted coefficients and selection frequency\n",
    "    coeffs = np.squeeze(pipeline._final_estimator.coef_)\n",
    "    dfCoefResults = pd.DataFrame({'Feature': XTrain.columns, 'Coeff': coeffs, 'Freq': coef_freq, 'ICC':iccList})\n",
    "    dfCoefResults.sort_values(by=['Coeff', 'Freq'], key=abs, inplace=True, ascending=False)\n",
    "\n",
    "    # print CV scores\n",
    "    AUC_CV = np.mean(cv_result['test_roc_auc'])\n",
    "    print('AUCROC   (CV) = ' + str(AUC_CV.round(3)))\n",
    "    print('Accuracy (CV) = ' + str(np.mean(cv_result['test_accuracy']).round(3)))\n",
    "    print('F1       (CV) = ' + str(np.mean(cv_result['test_f1']).round(3)))\n",
    "\n",
    "    # permutation testing\n",
    "    if permutationTest:\n",
    "        outer_cv.n_repeats = 1\n",
    "        scoreDirect, perm_scores, pValueDirect = permutation_test_score(pipeline, XTrain, yTrain, scoring=\"roc_auc\",\n",
    "                                                                        cv=outer_cv, n_permutations=n_permutations,\n",
    "                                                                        verbose=0, n_jobs=-1)\n",
    "\n",
    "        # pValueDirect is computed using scoreDirect and assumes only one outer CV run\n",
    "        # We have used repeated outer CV, so the following code correctly computes the p-value of our repeated CV performance estimate\n",
    "        # Actually, it doesn't seem to make much difference, so am relaxed about that.\n",
    "\n",
    "        p_values = []\n",
    "        scores_roc_auc = np.mean(np.reshape(cv_result['test_roc_auc'], (n_repeats, -1)), axis=1)\n",
    "        for score in scores_roc_auc:\n",
    "            p_values.append((np.count_nonzero(perm_scores >= score) + 1) / (n_permutations + 1))\n",
    "        print('p-value  (CV) = ' + str(np.mean(p_values).round(4)))\n",
    "\n",
    "    # get scores and predicted class info\n",
    "    y_pred_score_test = pipeline.predict_proba(XTest)[:, 1]\n",
    "    y_pred_class_test = pipeline.predict(XTest)\n",
    "\n",
    "    # test scores from main model\n",
    "    testAUROC = roc_auc_score(yTest, y_pred_score_test)\n",
    "    testAccuracy = accuracy_score(yTest, y_pred_class_test)\n",
    "    testF1 = f1_score(yTest, y_pred_class_test)\n",
    "\n",
    "    pValueTest = mannwhitneyu(y_pred_score_test[yTest==0], y_pred_score_test[yTest==1]).pvalue\n",
    "\n",
    "    # print the test performance metrics\n",
    "    print('\\nAUCROC   (test) = ' + str(np.round(testAUROC,3)))\n",
    "    print('Accuracy (test) = ' + str(np.round(testAccuracy,3)))\n",
    "    print('F1       (test) = ' + str(np.round(testF1,3)))\n",
    "    print('p-value  (test) = ' + str(np.round(pValueTest, 6)) + '\\n')\n",
    "\n",
    "    dfCoefResultsDisp = dfCoefResults.loc[dfCoefResults.Coeff != 0, :]\n",
    "    if displayCoef:\n",
    "        display(dfCoefResultsDisp.style.hide_index())\n",
    "\n",
    "    return AUC_CV, y_pred_score_test, y_pred_class_test, FNR, FPR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical and radiomics model (no semantic features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fitClinicalRadiomic(displayCoef=False, dropRegex=None, permutationTest=False):\n",
    "\n",
    "    print('\\n\\n\\nModel using clinical and radiomic features\\n')\n",
    "\n",
    "    # reproducible execution\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # get training and test data\n",
    "    XTrain = dfTrain.drop('ECE_Pathology', axis=1)\n",
    "    XTest = dfTest.drop('ECE_Pathology', axis=1)\n",
    "\n",
    "    # drop some features\n",
    "    XTrain.drop(list(XTrain.filter(regex = 'semantic')), axis = 1, inplace = True)\n",
    "    XTest.drop(list(XTest.filter(regex = 'semantic')), axis = 1, inplace = True)\n",
    "    if dropRegex is not None:\n",
    "        XTrain.drop(list(XTrain.filter(regex = dropRegex)), axis = 1, inplace = True)\n",
    "        XTest.drop(list(XTest.filter(regex = dropRegex)), axis = 1, inplace = True)\n",
    "    \n",
    "    # drop low variance features\n",
    "    CoV = pd.DataFrame(XTrain.apply(lambda x: np.std(x)/np.mean(x), axis=0))\n",
    "    lowVarianceFeatures = CoV.loc[CoV.loc[:,0]<0.01,:].index.values.tolist()\n",
    "    XTrain.drop(lowVarianceFeatures, axis = 1, inplace = True)\n",
    "    XTest.drop(lowVarianceFeatures, axis = 1, inplace = True)\n",
    "\n",
    "    # make sure the clinical features are not removed by the correlation step\n",
    "    namedColumnsKeep = [x for x in XTrain.columns if 'clinical' in x]\n",
    "\n",
    "    correlationHierarchy = ['shape_MeshVolume', 'shape'] #, 'firstorder']\n",
    "\n",
    "    model = LogisticRegression(solver=\"liblinear\", max_iter=10000, penalty='l1')\n",
    "    C_grid = np.logspace(np.log10(0.05), np.log10(50), 10)\n",
    "    inner_cv = StratifiedKFold(n_splits=5)\n",
    "    modelOpt = GridSearchCV(estimator=model, param_grid={'C':C_grid}, cv=inner_cv, refit=True, verbose=0, scoring='neg_log_loss', n_jobs=1)\n",
    "\n",
    "    pipeline = Pipeline([('correlationSelector', featureSelection_correlation(threshold=0.9,\n",
    "                                                                                      exact=False,\n",
    "                                                                                      featureGroupHierarchy=correlationHierarchy)),\n",
    "                                 ('scaler', StandardScaler()),\n",
    "                                 ('lr', modelOpt)])\n",
    "\n",
    "    # fit to all data\n",
    "    pipeline.fit(XTrain, yTrain)\n",
    "\n",
    "    # cross-validate\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "    cv_result = cross_validate(pipeline, X=XTrain, y=yTrain, cv=outer_cv, scoring=scorers,\n",
    "                               return_estimator=True, verbose=0, n_jobs=-1)\n",
    "\n",
    "    FNR, FPR = unpack_scorers(cv_result)\n",
    "\n",
    "    # get frequency that features are non-zero across the repeated cv splits\n",
    "    coef_cv = np.zeros((len(cv_result['estimator']), XTrain.shape[1]))\n",
    "    fs_mask = np.zeros((len(cv_result['estimator']), XTrain.shape[1]))\n",
    "    for n, res in enumerate(cv_result['estimator']):\n",
    "        fs_mask[n, :] = res.steps[0][1].mask_\n",
    "        coef_cv[n, res.steps[0][1].mask_] = res._final_estimator.best_estimator_.coef_\n",
    "    coef_freq = np.sum(coef_cv != 0, axis=0) / (n_repeats * n_splits)\n",
    "\n",
    "    # put icc values in array for including in DataFrame\n",
    "    iccList = []\n",
    "    for feat in XTrain.columns:\n",
    "        if feat in iccDict:\n",
    "            iccList.append(iccDict[feat])\n",
    "        else:\n",
    "            iccList.append('-')\n",
    "\n",
    "    # display sorted coefficients and selection frequency\n",
    "    coeffs = np.zeros(XTrain.shape[1])\n",
    "    coeffs[pipeline.steps[0][1].mask_] = np.squeeze(pipeline._final_estimator.best_estimator_.coef_)\n",
    "\n",
    "    dfCoefResults = pd.DataFrame({'Feature': XTrain.columns, 'Coeff': coeffs, 'Freq': coef_freq, 'ICC':iccList})\n",
    "    dfCoefResults.sort_values(by=['Coeff', 'Freq'], key=abs, inplace=True, ascending=False)\n",
    "\n",
    "    # print CV scores\n",
    "    AUC_CV = np.mean(cv_result['test_roc_auc'])\n",
    "    print('AUCROC   (CV) = ' + str(AUC_CV.round(3)))\n",
    "    print('Accuracy (CV) = ' + str(np.mean(cv_result['test_accuracy']).round(3)))\n",
    "    print('F1       (CV) = ' + str(np.mean(cv_result['test_f1']).round(3)))\n",
    "\n",
    "    # permutation testing\n",
    "    if permutationTest:\n",
    "        outer_cv.n_repeats = 1\n",
    "        scoreDirect, perm_scores, pValueDirect = permutation_test_score(pipeline, XTrain, yTrain, scoring=\"roc_auc\",\n",
    "                                                                        cv=outer_cv, n_permutations=n_permutations,\n",
    "                                                                        verbose=0, n_jobs=n_jobs)\n",
    "\n",
    "        # pValueDirect is computed using scoreDirect and assumes only one outer CV run\n",
    "        # We have used repeated outer CV, so the following code correctly computes the p-value of our repeated CV performance estimate\n",
    "        # Actually, it doesn't seem to make much difference, so am relaxed about that.\n",
    "\n",
    "        p_values = []\n",
    "        scores_roc_auc = np.mean(np.reshape(cv_result['test_roc_auc'], (n_repeats, -1)), axis=1)\n",
    "        for score in scores_roc_auc:\n",
    "            p_values.append((np.count_nonzero(perm_scores >= score) + 1) / (n_permutations + 1))\n",
    "        print('p-value       = ' + str(np.mean(p_values).round(4)))\n",
    "\n",
    "    # get scores and predicted class info\n",
    "    y_pred_score_test = pipeline.predict_proba(XTest)[:, 1]\n",
    "    y_pred_class_test = pipeline.predict(XTest)\n",
    "\n",
    "    # test scores from main model\n",
    "    testAUROC = roc_auc_score(yTest, y_pred_score_test)\n",
    "    testAccuracy = accuracy_score(yTest, y_pred_class_test)\n",
    "    testF1 = f1_score(yTest, y_pred_class_test)\n",
    "\n",
    "    pValueTest = mannwhitneyu(y_pred_score_test[yTest==0], y_pred_score_test[yTest==1]).pvalue\n",
    "\n",
    "    # print the test performance metrics\n",
    "    print('\\nAUCROC   (test) = ' + str(np.round(testAUROC,3)))\n",
    "    print('Accuracy (test) = ' + str(np.round(testAccuracy,3)))\n",
    "    print('F1       (test) = ' + str(np.round(testF1,3)))\n",
    "    print('p-value  (test) = ' + str(np.round(pValueTest, 6)) + '\\n')\n",
    "\n",
    "    dfCoefResultsDisp = dfCoefResults.loc[dfCoefResults.Coeff != 0, :]\n",
    "    if displayCoef:\n",
    "        display(dfCoefResultsDisp.style.hide_index())\n",
    "\n",
    "    return AUC_CV, y_pred_score_test, y_pred_class_test, FNR, FPR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical, semantic and radiomics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fitClinicalSemanticRadiomic(displayCoef=False, dropRegex=None, permutationTest=False):\n",
    "\n",
    "    print('\\n\\n\\nModel using clinical, semantic and radiomic features\\n')\n",
    "\n",
    "    # reproducible execution\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # get training and test data\n",
    "    XTrain = dfTrain.drop('ECE_Pathology', axis=1)\n",
    "    XTest = dfTest.drop('ECE_Pathology', axis=1)\n",
    "\n",
    "    # drop some features\n",
    "    if dropRegex is not None:\n",
    "        XTrain.drop(list(XTrain.filter(regex = dropRegex)), axis = 1, inplace = True)\n",
    "        XTest.drop(list(XTest.filter(regex = dropRegex)), axis = 1, inplace = True)\n",
    "\n",
    "    # drop low variance features\n",
    "    CoV = pd.DataFrame(XTrain.apply(lambda x: np.std(x)/np.mean(x), axis=0))\n",
    "    lowVarianceFeatures = CoV.loc[CoV.loc[:,0]<0.01,:].index.values.tolist()\n",
    "    XTrain.drop(lowVarianceFeatures, axis = 1, inplace = True)\n",
    "    XTest.drop(lowVarianceFeatures, axis = 1, inplace = True)\n",
    "\n",
    "    # make sure the clinical features are not removed by the correlation step\n",
    "    namedColumnsKeep = [x for x in XTrain.columns if 'clinical' in x]\n",
    "\n",
    "    correlationHierarchy = ['shape_MeshVolume', 'shape'] #, 'firstorder']\n",
    "\n",
    "    model = LogisticRegression(solver=\"liblinear\", max_iter=10000, penalty='l1')\n",
    "    C_grid = np.logspace(np.log10(0.05), np.log10(50), 10)\n",
    "    inner_cv = StratifiedKFold(n_splits=5)\n",
    "    modelOpt = GridSearchCV(estimator=model, param_grid={'C':C_grid}, cv=inner_cv, refit=True, verbose=0, scoring='neg_log_loss', n_jobs=1)\n",
    "\n",
    "    pipeline = Pipeline([('correlationSelector', featureSelection_correlation(threshold=0.9,\n",
    "                                                                                      exact=False,\n",
    "                                                                                      featureGroupHierarchy=correlationHierarchy)),\n",
    "                                 ('scaler', StandardScaler()),\n",
    "                                 ('lr', modelOpt)])\n",
    "\n",
    "    # fit to all data\n",
    "    pipeline.fit(XTrain, yTrain)\n",
    "\n",
    "    # cross-validate\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "    cv_result = cross_validate(pipeline, X=XTrain, y=yTrain, cv=outer_cv, scoring=scorers,\n",
    "                               return_estimator=True, verbose=0, n_jobs=-1)\n",
    "\n",
    "    FNR, FPR = unpack_scorers(cv_result)\n",
    "\n",
    "    # get frequency that features are non-zero across the repeated cv splits\n",
    "    coef_cv = np.zeros((len(cv_result['estimator']), XTrain.shape[1]))\n",
    "    fs_mask = np.zeros((len(cv_result['estimator']), XTrain.shape[1]))\n",
    "    for n, res in enumerate(cv_result['estimator']):\n",
    "        fs_mask[n, :] = res.steps[0][1].mask_\n",
    "        coef_cv[n, res.steps[0][1].mask_] = res._final_estimator.best_estimator_.coef_\n",
    "    coef_freq = np.sum(coef_cv != 0, axis=0) / (n_repeats * n_splits)\n",
    "\n",
    "    # put icc values in array for including in DataFrame\n",
    "    iccList = []\n",
    "    for feat in XTrain.columns:\n",
    "        if feat in iccDict:\n",
    "            iccList.append(iccDict[feat])\n",
    "        else:\n",
    "            iccList.append('-')\n",
    "\n",
    "    # display sorted coefficients and selection frequency\n",
    "    coeffs = np.zeros(XTrain.shape[1])\n",
    "    coeffs[pipeline.steps[0][1].mask_] = np.squeeze(pipeline._final_estimator.best_estimator_.coef_)\n",
    "\n",
    "    dfCoefResults = pd.DataFrame({'Feature': XTrain.columns, 'Coeff': coeffs, 'Freq': coef_freq, 'ICC':iccList})\n",
    "    dfCoefResults.sort_values(by=['Coeff', 'Freq'], key=abs, inplace=True, ascending=False)\n",
    "\n",
    "    # print CV scores\n",
    "    AUC_CV = np.mean(cv_result['test_roc_auc'])\n",
    "    print('AUCROC   (CV) = ' + str(AUC_CV.round(3)))\n",
    "    print('Accuracy (CV) = ' + str(np.mean(cv_result['test_accuracy']).round(3)))\n",
    "    print('F1       (CV) = ' + str(np.mean(cv_result['test_f1']).round(3)))\n",
    "\n",
    "    # permutation testing\n",
    "    if permutationTest:\n",
    "        outer_cv.n_repeats = 1\n",
    "        scoreDirect, perm_scores, pValueDirect = permutation_test_score(pipeline, XTrain, yTrain, scoring=\"roc_auc\",\n",
    "                                                                        cv=outer_cv, n_permutations=n_permutations,\n",
    "                                                                        verbose=0, n_jobs=n_jobs)\n",
    "\n",
    "        # pValueDirect is computed using scoreDirect and assumes only one outer CV run\n",
    "        # We have used repeated outer CV, so the following code correctly computes the p-value of our repeated CV performance estimate\n",
    "        # Actually, it doesn't seem to make much difference, so am relaxed about that.\n",
    "\n",
    "        p_values = []\n",
    "        scores_roc_auc = np.mean(np.reshape(cv_result['test_roc_auc'], (n_repeats, -1)), axis=1)\n",
    "        for score in scores_roc_auc:\n",
    "            p_values.append((np.count_nonzero(perm_scores >= score) + 1) / (n_permutations + 1))\n",
    "        print('p-value       = ' + str(np.mean(p_values).round(4)))\n",
    "\n",
    "    # get scores and predicted class info\n",
    "    y_pred_score_test = pipeline.predict_proba(XTest)[:, 1]\n",
    "    y_pred_class_test = pipeline.predict(XTest)\n",
    "\n",
    "    # test scores from main model\n",
    "    testAUROC = roc_auc_score(yTest, y_pred_score_test)\n",
    "    testAccuracy = accuracy_score(yTest, y_pred_class_test)\n",
    "    testF1 = f1_score(yTest, y_pred_class_test)\n",
    "\n",
    "    pValueTest = mannwhitneyu(y_pred_score_test[yTest==0], y_pred_score_test[yTest==1]).pvalue\n",
    "\n",
    "    # print the test performance metrics\n",
    "    print('\\nAUCROC   (test) = ' + str(np.round(testAUROC,3)))\n",
    "    print('Accuracy (test) = ' + str(np.round(testAccuracy,3)))\n",
    "    print('F1       (test) = ' + str(np.round(testF1,3)))\n",
    "    print('p-value  (test) = ' + str(np.round(pValueTest, 6)) + '\\n')\n",
    "\n",
    "    dfCoefResultsDisp = dfCoefResults.loc[dfCoefResults.Coeff != 0, :]\n",
    "    if displayCoef:\n",
    "        display(dfCoefResultsDisp.style.hide_index())\n",
    "\n",
    "    return AUC_CV, y_pred_score_test, y_pred_class_test, FNR, FPR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# used to remove texture features\n",
    "dropRegex = None #'glcm|gldm|glrlm|glszm|ngtdm'\n",
    "\n",
    "AUC_CV_clin, y_pred_score_test_clin, y_pred_class_test_clin, FNR_clin, FPR_clin = fitClinical()\n",
    "AUC_CV_clinSeman, y_pred_score_test_clinSeman, y_pred_class_test_clinSeman, FNR_clinSeman, FPR_clinSeman = fitClinicalSemantic()\n",
    "AUC_CV_clinRad, y_pred_score_test_clinRad, y_pred_class_test_clinRad, FNR_clinRad, FPR_clinRad = fitClinicalRadiomic(dropRegex=dropRegex)\n",
    "AUC_CV_clinSemRad, y_pred_score_test_clinSemRad, y_pred_class_test_clinSemRad, FNR_clinSemRad, FPR_clinSemRad = fitClinicalSemanticRadiomic(dropRegex=dropRegex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(22,12))\n",
    "ax = ax.ravel()\n",
    "\n",
    "ax[0].plot(FPR_clin['values'], [1-x for x in FNR_clin['values']], label = 'AUC = ' + str(np.round(AUC_CV_clin,3)) + ' Clinical')\n",
    "ax[0].plot(FPR_clinSeman['values'], [1-x for x in FNR_clinSeman['values']], label = 'AUC = ' + str(np.round(AUC_CV_clinSeman,3)) + ' Clinical + Semantic')\n",
    "ax[0].plot(FPR_clinRad['values'], [1-x for x in FNR_clinRad['values']], label = 'AUC = ' + str(np.round(AUC_CV_clinRad,3)) + ' Clinical + Radiomic')\n",
    "ax[0].plot(FPR_clinSemRad['values'], [1-x for x in FNR_clinSemRad['values']], label = 'AUC = ' + str(np.round(AUC_CV_clinSemRad,3)) + ' Clinical + Semantic + Radiomic')\n",
    "ax[0].set_xlabel('1 - Specificity', fontsize=14)\n",
    "ax[0].set_ylabel('Sensitivity', fontsize=14)\n",
    "ax[0].set_title('ROC', fontsize=16, fontweight='bold')\n",
    "ax[0].legend(fontsize=11)\n",
    "ax[0].text(-0.3, 0.5, 'Discovery data, cross-validated', fontsize=16, fontweight='bold', rotation=90, verticalalignment='center')\n",
    "\n",
    "ax[1].plot(FNR_clin['thresholds'], FNR_clin['values'], label='Clinical')\n",
    "ax[1].plot(FNR_clinSeman['thresholds'], FNR_clinSeman['values'], label='Clinical + Semantic')\n",
    "ax[1].plot(FNR_clinRad['thresholds'], FNR_clinRad['values'], label='Clinical + Radiomic')\n",
    "ax[1].plot(FNR_clinSemRad['thresholds'], FNR_clinSemRad['values'], label='Clinical + Semantic + Radiomic')\n",
    "ax[1].set_title('False negative rate', fontsize=16, fontweight='bold')\n",
    "ax[1].legend(fontsize=11)\n",
    "ax[1].set_xlabel('Threshold', fontsize=14)\n",
    "ax[1].set_ylabel('False negative rate', fontsize=14)\n",
    "\n",
    "ax[2].plot(FPR_clin['thresholds'], FPR_clin['values'], label='Clinical')\n",
    "ax[2].plot(FPR_clinSeman['thresholds'], FPR_clinSeman['values'], label='Clinical + Semantic')\n",
    "ax[2].plot(FPR_clinRad['thresholds'], FPR_clinRad['values'], label='Clinical + Radiomic')\n",
    "ax[2].plot(FPR_clinSemRad['thresholds'], FPR_clinSemRad['values'], label='Clinical + Semantic + Radiomic')\n",
    "ax[2].set_title('False positive rate', fontsize=16, fontweight='bold')\n",
    "ax[2].legend(fontsize=11)\n",
    "ax[2].set_xlabel('Threshold', fontsize=14)\n",
    "ax[2].set_ylabel('False positive rate', fontsize=14)\n",
    "\n",
    "# plot comparing ROCs\n",
    "fpr_clin, tpr_clin, _ = roc_curve(yTest, y_pred_score_test_clin)\n",
    "fpr_clinSeman, tpr_clinSeman, _ = roc_curve(yTest, y_pred_score_test_clinSeman)\n",
    "fpr_clinRad, tpr_clinRad, _ = roc_curve(yTest, y_pred_score_test_clinRad)\n",
    "fpr_clinSemRad, tpr_clinSemRad, _ = roc_curve(yTest, y_pred_score_test_clinSemRad)\n",
    "\n",
    "thresh = np.linspace(0, 1, 500)\n",
    "tnTest_clinical, fpTest_clinical, fnTest_clinical, tpTest_clinical = roc_curve_thresholds(yTest, y_pred_score_test_clin, thresh)\n",
    "tnTest_semantic, fpTest_semantic, fnTest_semantic, tpTest_semantic = roc_curve_thresholds(yTest, y_pred_score_test_clinSeman, thresh)\n",
    "tnTest_radiomic, fpTest_radiomic, fnTest_radiomic, tpTest_radiomic = roc_curve_thresholds(yTest, y_pred_score_test_clinRad, thresh)\n",
    "tnTest_semradic, fpTest_semradic, fnTest_semradic, tpTest_semradic = roc_curve_thresholds(yTest, y_pred_score_test_clinSemRad, thresh)\n",
    "\n",
    "ax[3].plot(fpr_clin, tpr_clin,             label='AUC = ' + str(np.round(roc_auc_score(yTest, y_pred_score_test_clin),3)) + '  Clinical')\n",
    "ax[3].plot(fpr_clinSeman, tpr_clinSeman,   label='AUC = ' + str(np.round(roc_auc_score(yTest, y_pred_score_test_clinSeman),3)) + '  Clinical + Semantic')\n",
    "ax[3].plot(fpr_clinRad, tpr_clinRad,       label='AUC = ' + str(np.round(roc_auc_score(yTest, y_pred_score_test_clinRad),3)) + '  Clinical + Radiomic')\n",
    "ax[3].plot(fpr_clinSemRad, tpr_clinSemRad, label='AUC = ' + str(np.round(roc_auc_score(yTest, y_pred_score_test_clinSemRad),3)) + '  Clinical + Semantic + Radiomic')\n",
    "ax[3].set_xlabel('1 - Specificity', fontsize=14)\n",
    "ax[3].set_ylabel('Sensitivity', fontsize=14)\n",
    "#ax[3].set_title('Test data', fontsize=14)\n",
    "ax[3].legend(fontsize=11)\n",
    "ax[3].text(-0.3, 0.5, 'Test data', fontsize=16, fontweight='bold', rotation=90, verticalalignment='center')\n",
    "\n",
    "ax[4].plot(thresh, fnTest_clinical/(fnTest_clinical + tpTest_clinical), label='Clinical')\n",
    "ax[4].plot(thresh, fnTest_semantic/(fnTest_semantic + tpTest_semantic), label='Clinical + Semantic')\n",
    "ax[4].plot(thresh, fnTest_radiomic/(fnTest_radiomic + tpTest_radiomic), label='Clinical + Radiomic')\n",
    "ax[4].plot(thresh, fnTest_semradic/(fnTest_semradic + tpTest_semradic), label='Clinical + Semantic + Radiomic')\n",
    "#ax[4].set_title('False negative rate', fontsize=16)\n",
    "ax[4].legend(fontsize=11)\n",
    "ax[4].set_xlabel('Threshold', fontsize=14)\n",
    "ax[4].set_ylabel('False negative rate', fontsize=14)\n",
    "\n",
    "ax[5].plot(thresh, fpTest_clinical/(fpTest_clinical + tnTest_clinical), label='Clinical')\n",
    "ax[5].plot(thresh, fpTest_semantic/(fpTest_semantic + tnTest_semantic), label='Clinical + Semantic')\n",
    "ax[5].plot(thresh, fpTest_radiomic/(fpTest_radiomic + tnTest_radiomic), label='Clinical + Radiomic')\n",
    "ax[5].plot(thresh, fpTest_semradic/(fpTest_semradic + tnTest_semradic), label='Clinical + Semantic + Radiomic')\n",
    "#ax[5].set_title('False positive rate', fontsize=16)\n",
    "ax[5].legend(fontsize=11)\n",
    "ax[5].set_xlabel('Threshold', fontsize=14)\n",
    "ax[5].set_ylabel('False positive rate', fontsize=14)\n",
    "\n",
    "figFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/results/Figure1.pdf')\n",
    "plt.savefig(figFile)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
