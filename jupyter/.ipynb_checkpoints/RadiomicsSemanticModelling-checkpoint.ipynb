{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import compress\n",
    "import copy, sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "# from featureSelect_correlation import featureSelect_correlation\n",
    "from pyirr import intraclass_correlation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, RepeatedStratifiedKFold, permutation_test_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve, confusion_matrix\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# use all the processors unless we are in debug mode\n",
    "n_jobs = -1\n",
    "if getattr(sys, 'gettrace', None)():\n",
    "    n_jobs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_two_reader_data(fileName):\n",
    "\n",
    "    # read spreadsheet\n",
    "    df = pd.read_excel(fileName, sheet_name='GG_MG', engine='openpyxl')\n",
    "\n",
    "    # remove features, as with the discovery/test data\n",
    "    df.drop(['IndexLesion_GG', 'IndexLesionMG', 'GlobalStageGG', 'GlobalStageMG'], axis=1, inplace=True)\n",
    "\n",
    "    # remove rows with missing data - need to check that this leaves the same patients for dfGG as in the discovery data set\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # split to each reader\n",
    "    dfGG = df.filter(regex = 'GG|PID', axis = 1)\n",
    "    dfMG = df.filter(regex='MG|PID', axis=1)\n",
    "\n",
    "    # match column names by removing subscripts\n",
    "    dfGG = dfGG.rename(columns=lambda x: x.replace('_GG','').replace('GG',''))\n",
    "    dfMG = dfMG.rename(columns=lambda x: x.replace('_MG','').replace('MG',''))\n",
    "\n",
    "    # change some column names to match the discovery/test data sets\n",
    "    renameDict = {'LocIndexL':'AnatDev01',\n",
    "                  'LocAnat':'AnatDev02',\n",
    "                  'Division':'AnatDev03',\n",
    "                  'DivisionLat':'AnatDev04',\n",
    "                  'LesionSize':'MajorLengthIndex',\n",
    "                  'SmoothCapsularBulgin':'SmoothCapsularBulging',\n",
    "                  'UnsharpMargins':'UnsharpMargin',\n",
    "                  'irregularContour':'IrregularContour',\n",
    "                  'BlackEstrition':'BlackEstritionPeripFat',\n",
    "                  'measurableECE':'MeasurableECE',\n",
    "                  'retroprostaticAngleObl':'RetroprostaticAngleOblit'}\n",
    "    dfGG.rename(renameDict, axis=1, inplace=True)\n",
    "    dfMG.rename(renameDict, axis=1, inplace=True)\n",
    "\n",
    "    # highsignalT1FS is missing from this spreadsheet, so fill in with default value.\n",
    "    # Fortunately, this feature is not selected in the final model, but we need it there for compatibility.\n",
    "    dfGG.loc[:, 'highsignalT1FS'] = 0\n",
    "    dfMG.loc[:, 'highsignalT1FS'] = 0\n",
    "\n",
    "    iccDict = {}\n",
    "    for col in dfGG.drop(['PID', 'highsignalT1FS'], axis=1):\n",
    "        data = np.stack((dfGG[col], dfMG[col]), axis=1)\n",
    "        iccDict[col] = intraclass_correlation(data, \"twoway\", \"agreement\").value\n",
    "\n",
    "    return dfGG, dfMG, iccDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_radiomics_data(radiomicsFile):\n",
    "\n",
    "    df = pd.read_csv(radiomicsFile)\n",
    "    df.drop(list(df.filter(regex = 'source')), axis = 1, inplace = True)\n",
    "    df.drop(list(df.filter(regex = 'diagnostics')), axis = 1, inplace = True)\n",
    "\n",
    "    # To match the semantic data file\n",
    "    df['StudyPatientName'] = df['StudyPatientName'].str.replace('_',' ')\n",
    "    \n",
    "    # split off the repro rows\n",
    "    dfRep1 = df.loc[df.StudyPatientName.str.contains('rep'),:].copy()\n",
    "    dfRep1['StudyPatientName'] = dfRep1['StudyPatientName'].str.replace(' repro','')\n",
    "    dfRep1.sort_values('StudyPatientName', axis=0, inplace=True)\n",
    "    dfRep1.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # remove repro from main data frame\n",
    "    df = df.loc[~df.StudyPatientName.str.contains('rep'),:]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # main data rows for same patients as repro\n",
    "    dfRep0 = df.loc[df['StudyPatientName'].isin(dfRep1['StudyPatientName'])].copy()\n",
    "    dfRep0.sort_values('StudyPatientName', axis=0, inplace=True)\n",
    "    dfRep0.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    iccDict = {}\n",
    "    for col in dfRep1.drop('StudyPatientName', axis=1):\n",
    "        data = np.stack((dfRep0[col], dfRep1[col]), axis=1)\n",
    "        iccDict[col] = intraclass_correlation(data, \"twoway\", \"agreement\").value\n",
    "\n",
    "    return df, iccDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile):\n",
    "    \n",
    "   # load data\n",
    "    dfTrain = pd.read_csv(discoveryFile)\n",
    "    dfTest  = pd.read_csv(externalTestFile)\n",
    "    dfGG, dfMG, iccDictSemantic = load_two_reader_data(twoReaderFile)\n",
    "    dfRad, iccDictRadiomics = load_radiomics_data(radiomicsFile)\n",
    "    \n",
    "    # drop features we are not going to use for classification\n",
    "    dfTrain.drop(['Gleason biopsy','TumorGradeMRI'], inplace=True, axis=1)\n",
    "    dfTest.drop(['Gleason biopsy','TumorGradeMRI'], inplace=True, axis=1)\n",
    "\n",
    "    # merge radiomics \n",
    "    dfTrain.merge(dfRad, left_on='PID', right_on='StudyPatientName')\n",
    "    dfTest.merge(dfRad, left_on='PID', right_on='StudyPatientName')\n",
    "    print(dfTest.PID)\n",
    "    \n",
    "    # merge with clinical features from training data\n",
    "    featuresFromTrainingData = ['PID', 'GleasonBinary', 'ProstateVolume', 'PSA', 'IndLesPIRADS_V2', 'ECE_Pathology']\n",
    "    dfGG = dfGG.merge(dfTrain[featuresFromTrainingData], on='PID')\n",
    "    dfMG = dfMG.merge(dfTrain[featuresFromTrainingData], on='PID')\n",
    "\n",
    "    # make sure columns are ordered the same\n",
    "    dfGG = dfGG[dfTrain.columns]\n",
    "    dfMG = dfMG[dfTrain.columns]\n",
    "\n",
    "    # make these features binary 0/1\n",
    "    toBinary = ['SmoothCapsularBulging' ,'CapsularDisruption', 'UnsharpMargin', 'IrregularContour', 'BlackEstritionPeripFat', 'MeasurableECE', 'RetroprostaticAngleOblit', 'highsignalT1FS']\n",
    "    for tb in toBinary:\n",
    "        dfTrain[tb]  = dfTrain[tb].map(dict(YES=1, NO=0))\n",
    "        dfTest[tb] = dfTest[tb].map(dict(YES=1, NO=0))\n",
    "\n",
    "    # is missing in test and training, so replace both with median from the training data\n",
    "    psaTrainMedian = np.nanmedian(np.array(dfTrain.PSA))\n",
    "    dfTrain.PSA.fillna(psaTrainMedian, inplace=True)\n",
    "    dfTest.PSA.fillna(psaTrainMedian, inplace=True)\n",
    "\n",
    "    # this feature is not selected in the semantic model, so this has no effect\n",
    "    # fill in with the most common value\n",
    "    dfTest.highsignalT1FS.fillna(0, inplace=True)\n",
    "\n",
    "    # extract data into numpy arrays, but keep the feature names\n",
    "    yTrain = np.array(dfTrain.ECE_Pathology)\n",
    "    yTest = np.array(dfTest.ECE_Pathology)\n",
    "    yMG_GG = np.array(dfGG.ECE_Pathology)\n",
    "\n",
    "    XTrain = dfTrain.drop(['PID', 'ECE_Pathology'], axis=1)\n",
    "    XTest = dfTest.drop(['PID', 'ECE_Pathology'], axis=1)\n",
    "    X_GG = dfGG.drop(['PID', 'ECE_Pathology'], axis=1)\n",
    "    X_MG = dfMG.drop(['PID', 'ECE_Pathology'], axis=1)\n",
    "\n",
    "    featureNames = list(XTrain.columns)\n",
    "    XTrain = np.array(XTrain)\n",
    "    XTest = np.array(XTest)\n",
    "    X_GG = np.array(X_GG)\n",
    "    X_MG = np.array(X_MG)\n",
    "\n",
    "    return dfTrain, dfRad #XTrain, yTrain, XTest, yTest, X_GG, X_MG, yMG_GG, featureNames, iccDictSemantic, iccDictRadiomics, dfRad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_training_data(XTrain, yTrain, featureNames, iccDict, n_repeats=10, n_splits=10, n_permutations=10, crossValidateFit=True, printResubstitutionMetrics=False):\n",
    "\n",
    "    # Check for data leak - this is in case the way this code is split into various functions\n",
    "    # accidentally allows scope of the test data to include function.\n",
    "    if 'yTest' in locals() or 'yTest' in globals():\n",
    "        print('Test data is accessible to training function - check code for data leak!!!')\n",
    "        return {}, None\n",
    "\n",
    "    # reproducible execution\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # logistic LASSO tuning parameter optimised using function with in-built CV\n",
    "    pipeline = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                               ('logistic',LogisticRegressionCV(Cs=20, \n",
    "                                                                cv=10, \n",
    "                                                                solver=\"liblinear\",\n",
    "                                                                max_iter=10000, penalty='l1',\n",
    "                                                                random_state=seed))])\n",
    "\n",
    "    # fit to all data\n",
    "    pipeline.fit(XTrain, yTrain)\n",
    "\n",
    "    # print some performance metrics\n",
    "    if printResubstitutionMetrics:\n",
    "        y_pred_score = pipeline.predict_proba(XTrain)[:, 1]\n",
    "        y_pred_class = pipeline.predict(XTrain)\n",
    "        resubAUROC = roc_auc_score(yTrain, y_pred_score)\n",
    "        resubAccuracy = accuracy_score(yTrain, y_pred_class)\n",
    "        resubF1 = f1_score(yTrain, y_pred_class)\n",
    "\n",
    "        print('AUCROC  (resub) = ' + str(np.round(resubAUROC, 3)))\n",
    "        print('Accuracy (resub) = ' + str(np.round(resubAccuracy, 3)))\n",
    "        print('F1 (resub) = ' + str(np.round(resubF1, 3)))\n",
    "        print(' ')\n",
    "\n",
    "    # default value for this when crossValidateFit = False\n",
    "    dfCoefResults = None\n",
    "    \n",
    "    if crossValidateFit:\n",
    "\n",
    "        # cross-validate\n",
    "        outer_cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "        cv_result = cross_validate(pipeline, X=XTrain, y=yTrain, cv=outer_cv, scoring=['accuracy', 'roc_auc', 'f1'],\n",
    "                                   return_estimator=True, verbose=0, n_jobs=n_jobs)\n",
    "\n",
    "        # get frequency that features are non-zero across the repeated cv splits\n",
    "        coef_cv = np.zeros((len(cv_result['estimator']), XTrain.shape[1]))\n",
    "        for n, res in enumerate(cv_result['estimator']):\n",
    "            coef_cv[n, :] = res._final_estimator.coef_\n",
    "        coef_freq = np.sum(coef_cv != 0, axis=0) / (n_repeats * n_splits)\n",
    "\n",
    "        # put icc values in array for including in DataFrame\n",
    "        iccList = []\n",
    "        for feat in featureNames:\n",
    "            if feat in iccDict:\n",
    "                iccList.append(iccDict[feat])\n",
    "            else:\n",
    "                iccList.append('-')\n",
    "\n",
    "        # display sorted coefficients and selection frequency\n",
    "        coeffs = np.squeeze(pipeline._final_estimator.coef_)\n",
    "        dfCoefResults = pd.DataFrame({'Feature': featureNames, 'Coeff': coeffs, 'Freq': coef_freq, 'ICC':iccList})\n",
    "        dfCoefResults.sort_values(by=['Coeff', 'Freq'], key=abs, inplace=True, ascending=False)\n",
    "\n",
    "        # print CV scores\n",
    "        print('AUCROC   (CV) = ' + str(np.mean(cv_result['test_roc_auc']).round(3)))\n",
    "        print('Accuracy (CV) = ' + str(np.mean(cv_result['test_accuracy']).round(3)))\n",
    "        print('F1       (CV) = ' + str(np.mean(cv_result['test_f1']).round(3)))\n",
    "\n",
    "        # permutation testing\n",
    "        outer_cv.n_repeats = 1\n",
    "        scoreDirect, perm_scores, pValueDirect = permutation_test_score(pipeline, XTrain, yTrain, scoring=\"roc_auc\",\n",
    "                                                                        cv=outer_cv, n_permutations=n_permutations,\n",
    "                                                                        verbose=0, n_jobs=n_jobs)\n",
    "\n",
    "        # pValueDirect is computed using scoreDirect and assumes only one outer CV run\n",
    "        # We have used repeated outer CV, so the following code correctly computes the p-value of our repeated CV performance estimate\n",
    "        # Actually, it doesn't seem to make much difference, so am relaxed about that.\n",
    "\n",
    "        p_values = []\n",
    "        scores_roc_auc = np.mean(np.reshape(cv_result['test_roc_auc'], (n_repeats, -1)), axis=1)\n",
    "        for score in scores_roc_auc:\n",
    "            p_values.append((np.count_nonzero(perm_scores >= score) + 1) / (n_permutations + 1))\n",
    "        print('p-value       = ' + str(np.mean(p_values).round(4)) + '\\n\\n')\n",
    "\n",
    "    return pipeline, dfCoefResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fit_training_test(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile, n_splits=10, n_repeats=10, n_permutations=10):\n",
    "\n",
    "    # get all required data arrays\n",
    "    XTrain, yTrain, XTest, yTest, X_GG, X_MG, yMG_GG, featureNames, iccDictSemantic, iccDictRadiomics, dfRad = load_data(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile)\n",
    "\n",
    "    # fit training data using cross-validation and permutation testing\n",
    "    pipeline, dfCoefResults = fit_training_data(XTrain, yTrain, featureNames, iccDictSemantic, \n",
    "                               n_splits=n_splits, n_repeats=n_repeats, \n",
    "                               n_permutations=n_permutations, crossValidateFit=True)\n",
    "\n",
    "    # re-fit training data using features selected based on the frequency \n",
    "    # the logisticLASSO retains each feature being > 0.9\n",
    "    selectedFeatures = ['GleasonBinary', 'MeasurableECE', 'CapsularContactLength', \n",
    "                        'IrregularContour', 'CapsularDisruption']\n",
    "    XTrain_reducedModel = np.array(pd.DataFrame(XTrain, columns=featureNames)[selectedFeatures])\n",
    "    XTest_reducedModel = np.array(pd.DataFrame(XTest, columns=featureNames)[selectedFeatures])\n",
    "    #\n",
    "    pipeline_reducedModel, _ = fit_training_data(XTrain_reducedModel, yTrain, featureNames, iccDictSemantic, crossValidateFit=False)\n",
    "\n",
    "    # package lots of variables up into a dict for tidy output\n",
    "    outputVars = ('pipeline', 'dfCoefResults', 'pipeline_reducedModel', 'XTrain', 'XTrain_reducedModel', \n",
    "                  'yTrain', 'XTest', 'XTest_reducedModel', 'yTest', 'X_GG', 'X_MG', 'yMG_GG')\n",
    "    out = {}\n",
    "    for var in outputVars:\n",
    "        out[var] = locals()[var]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     IAP 238\n",
      "1     IAP 239\n",
      "2     IAP 240\n",
      "3     IAP 241\n",
      "4     IAP 242\n",
      "5     IAP 246\n",
      "6     IAP 247\n",
      "7     IAP 249\n",
      "8     IAP 250\n",
      "9     IAP 251\n",
      "10    IAP 256\n",
      "11    IAP 258\n",
      "12    IAP 259\n",
      "13    IAP 260\n",
      "14    IAP 263\n",
      "15    IAP 266\n",
      "16    IAP 267\n",
      "17    IAP 268\n",
      "18    IAP 269\n",
      "19    IAP 271\n",
      "20    IAP 272\n",
      "21    IAP 273\n",
      "22    IAP 276\n",
      "23    IAP 278\n",
      "24    IAP 279\n",
      "25    IAP 280\n",
      "26    IAP 281\n",
      "27    IAP 282\n",
      "28    IAP 283\n",
      "29    IAP 284\n",
      "30    IAP 285\n",
      "31    IAP 286\n",
      "32    IAP 287\n",
      "33    IAP 288\n",
      "34    IAP 289\n",
      "35    IAP 290\n",
      "36    IAP 292\n",
      "37    IAP 293\n",
      "38    IAP 296\n",
      "39    IAP 298\n",
      "40    IAP 243\n",
      "41    IAP 244\n",
      "42    IAP 245\n",
      "43    IAP 248\n",
      "44    IAP 252\n",
      "45    IAP 253\n",
      "46    IAP 254\n",
      "47    IAP 255\n",
      "48    IAP 257\n",
      "49    IAP 262\n",
      "50    IAP 265\n",
      "51    IAP 270\n",
      "52    IAP 274\n",
      "53    IAP 275\n",
      "54    IAP 277\n",
      "55    IAP 291\n",
      "56    IAP 294\n",
      "57    IAP 295\n",
      "58    IAP 297\n",
      "Name: PID, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load data and fit models\n",
    "\n",
    "discoveryFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/discovery.csv')\n",
    "externalTestFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/external.csv')\n",
    "twoReaderFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/GG_MG.xlsx')\n",
    "radiomicsFile = os.path.join(os.path.expanduser('~'), 'Dropbox (ICR)/CLINMAG/Radiomics/ECE_Prostate_Semantic/ECE_Semantic_Data/radiomicFeatures.csv')\n",
    "\n",
    "n_splits = 10\n",
    "n_repeats = 1 #100\n",
    "n_permutations = 10 #1000\n",
    "\n",
    "# result = load_fit_training_test(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile,\n",
    "#                                 n_splits=n_splits, n_repeats=n_repeats, n_permutations=n_permutations)\n",
    "\n",
    "#XTrain, yTrain, XTest, yTest, X_GG, X_MG, yMG_GG, featureNames, iccDictSemantic, iccDictRadiomics, dfRad = load_data(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile)\n",
    "dfTrain, dfRad = load_data(discoveryFile, externalTestFile, twoReaderFile, radiomicsFile)\n",
    "\n",
    "\n",
    "# unpack dictionary to variables\n",
    "# locals().update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>GleasonBinary</th>\n",
       "      <th>ProstateVolume</th>\n",
       "      <th>PSA</th>\n",
       "      <th>IndLesPIRADS_V2</th>\n",
       "      <th>AnatDev01</th>\n",
       "      <th>AnatDev02</th>\n",
       "      <th>AnatDev03</th>\n",
       "      <th>AnatDev04</th>\n",
       "      <th>MajorLengthIndex</th>\n",
       "      <th>...</th>\n",
       "      <th>original_histogram_40Percentile</th>\n",
       "      <th>original_histogram_45Percentile</th>\n",
       "      <th>original_histogram_55Percentile</th>\n",
       "      <th>original_histogram_60Percentile</th>\n",
       "      <th>original_histogram_65Percentile</th>\n",
       "      <th>original_histogram_70Percentile</th>\n",
       "      <th>original_histogram_75Percentile</th>\n",
       "      <th>original_histogram_80Percentile</th>\n",
       "      <th>original_histogram_85Percentile</th>\n",
       "      <th>original_histogram_95Percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IAP 001</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>8.90</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.345272</td>\n",
       "      <td>-0.248641</td>\n",
       "      <td>-0.087590</td>\n",
       "      <td>-0.001696</td>\n",
       "      <td>0.094935</td>\n",
       "      <td>0.245249</td>\n",
       "      <td>0.406301</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>1.984603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IAP 002</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5.90</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224335</td>\n",
       "      <td>-0.107233</td>\n",
       "      <td>0.097695</td>\n",
       "      <td>0.166004</td>\n",
       "      <td>0.275787</td>\n",
       "      <td>0.390449</td>\n",
       "      <td>0.553904</td>\n",
       "      <td>0.712479</td>\n",
       "      <td>0.956442</td>\n",
       "      <td>1.737121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IAP 003</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320574</td>\n",
       "      <td>-0.223365</td>\n",
       "      <td>-0.041099</td>\n",
       "      <td>0.068261</td>\n",
       "      <td>0.177622</td>\n",
       "      <td>0.286982</td>\n",
       "      <td>0.432795</td>\n",
       "      <td>0.578608</td>\n",
       "      <td>0.773026</td>\n",
       "      <td>1.672209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IAP 004</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>3.17</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.384406</td>\n",
       "      <td>-0.304034</td>\n",
       "      <td>-0.108845</td>\n",
       "      <td>-0.005509</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.224125</td>\n",
       "      <td>0.407832</td>\n",
       "      <td>0.625985</td>\n",
       "      <td>0.926806</td>\n",
       "      <td>2.132386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IAP 005</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>7.10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312689</td>\n",
       "      <td>-0.197227</td>\n",
       "      <td>0.062563</td>\n",
       "      <td>0.206890</td>\n",
       "      <td>0.322352</td>\n",
       "      <td>0.466679</td>\n",
       "      <td>0.611007</td>\n",
       "      <td>0.841931</td>\n",
       "      <td>1.043989</td>\n",
       "      <td>1.794492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>IAP 232</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>12.50</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271078</td>\n",
       "      <td>-0.171971</td>\n",
       "      <td>0.040402</td>\n",
       "      <td>0.167825</td>\n",
       "      <td>0.281090</td>\n",
       "      <td>0.398603</td>\n",
       "      <td>0.535937</td>\n",
       "      <td>0.705835</td>\n",
       "      <td>0.918207</td>\n",
       "      <td>1.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>IAP 233</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.350934</td>\n",
       "      <td>-0.269951</td>\n",
       "      <td>-0.134979</td>\n",
       "      <td>-0.053997</td>\n",
       "      <td>0.026986</td>\n",
       "      <td>0.134964</td>\n",
       "      <td>0.269935</td>\n",
       "      <td>0.526381</td>\n",
       "      <td>0.823319</td>\n",
       "      <td>2.056284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>IAP 234</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.274925</td>\n",
       "      <td>-0.189821</td>\n",
       "      <td>0.022938</td>\n",
       "      <td>0.133573</td>\n",
       "      <td>0.320801</td>\n",
       "      <td>0.491008</td>\n",
       "      <td>0.576112</td>\n",
       "      <td>0.780361</td>\n",
       "      <td>1.001630</td>\n",
       "      <td>1.818625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>IAP 236</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276395</td>\n",
       "      <td>-0.190564</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.152763</td>\n",
       "      <td>0.276992</td>\n",
       "      <td>0.446397</td>\n",
       "      <td>0.627095</td>\n",
       "      <td>0.823604</td>\n",
       "      <td>1.033665</td>\n",
       "      <td>1.835512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>IAP 237</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>5.30</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.319380</td>\n",
       "      <td>-0.204384</td>\n",
       "      <td>0.025607</td>\n",
       "      <td>0.140602</td>\n",
       "      <td>0.255598</td>\n",
       "      <td>0.408925</td>\n",
       "      <td>0.562253</td>\n",
       "      <td>0.753912</td>\n",
       "      <td>0.983903</td>\n",
       "      <td>1.750540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PID  GleasonBinary  ProstateVolume    PSA  IndLesPIRADS_V2  \\\n",
       "0    IAP 001              0              44   8.90                5   \n",
       "1    IAP 002              0              27   5.90                4   \n",
       "2    IAP 003              1              42   3.60                5   \n",
       "3    IAP 004              0              33   3.17                5   \n",
       "4    IAP 005              0              32   7.10                4   \n",
       "..       ...            ...             ...    ...              ...   \n",
       "134  IAP 232              1             122  12.50                3   \n",
       "135  IAP 233              0              53  10.00                4   \n",
       "136  IAP 234              0              72   4.60                4   \n",
       "137  IAP 236              0              35   3.40                3   \n",
       "138  IAP 237              1              42   5.30                5   \n",
       "\n",
       "     AnatDev01  AnatDev02  AnatDev03  AnatDev04  MajorLengthIndex  ...  \\\n",
       "0            0          0          1          0                15  ...   \n",
       "1            1          0          1          2                10  ...   \n",
       "2            5          0          1          1                18  ...   \n",
       "3            1          0          1          1                15  ...   \n",
       "4            4          0          1          2                 9  ...   \n",
       "..         ...        ...        ...        ...               ...  ...   \n",
       "134          0          0          1          2                17  ...   \n",
       "135          4          0          1          1                10  ...   \n",
       "136          0          0          1          2                 8  ...   \n",
       "137          4          0          1          2                13  ...   \n",
       "138          3          0          2          2                20  ...   \n",
       "\n",
       "     original_histogram_40Percentile  original_histogram_45Percentile  \\\n",
       "0                          -0.345272                        -0.248641   \n",
       "1                          -0.224335                        -0.107233   \n",
       "2                          -0.320574                        -0.223365   \n",
       "3                          -0.384406                        -0.304034   \n",
       "4                          -0.312689                        -0.197227   \n",
       "..                               ...                              ...   \n",
       "134                        -0.271078                        -0.171971   \n",
       "135                        -0.350934                        -0.269951   \n",
       "136                        -0.274925                        -0.189821   \n",
       "137                        -0.276395                        -0.190564   \n",
       "138                        -0.319380                        -0.204384   \n",
       "\n",
       "     original_histogram_55Percentile  original_histogram_60Percentile  \\\n",
       "0                          -0.087590                        -0.001696   \n",
       "1                           0.097695                         0.166004   \n",
       "2                          -0.041099                         0.068261   \n",
       "3                          -0.108845                        -0.005509   \n",
       "4                           0.062563                         0.206890   \n",
       "..                               ...                              ...   \n",
       "134                         0.040402                         0.167825   \n",
       "135                        -0.134979                        -0.053997   \n",
       "136                         0.022938                         0.133573   \n",
       "137                         0.028533                         0.152763   \n",
       "138                         0.025607                         0.140602   \n",
       "\n",
       "     original_histogram_65Percentile  original_histogram_70Percentile  \\\n",
       "0                           0.094935                         0.245249   \n",
       "1                           0.275787                         0.390449   \n",
       "2                           0.177622                         0.286982   \n",
       "3                           0.097826                         0.224125   \n",
       "4                           0.322352                         0.466679   \n",
       "..                               ...                              ...   \n",
       "134                         0.281090                         0.398603   \n",
       "135                         0.026986                         0.134964   \n",
       "136                         0.320801                         0.491008   \n",
       "137                         0.276992                         0.446397   \n",
       "138                         0.255598                         0.408925   \n",
       "\n",
       "     original_histogram_75Percentile  original_histogram_80Percentile  \\\n",
       "0                           0.406301                         0.578089   \n",
       "1                           0.553904                         0.712479   \n",
       "2                           0.432795                         0.578608   \n",
       "3                           0.407832                         0.625985   \n",
       "4                           0.611007                         0.841931   \n",
       "..                               ...                              ...   \n",
       "134                         0.535937                         0.705835   \n",
       "135                         0.269935                         0.526381   \n",
       "136                         0.576112                         0.780361   \n",
       "137                         0.627095                         0.823604   \n",
       "138                         0.562253                         0.753912   \n",
       "\n",
       "     original_histogram_85Percentile  original_histogram_95Percentile  \n",
       "0                           0.857244                         1.984603  \n",
       "1                           0.956442                         1.737121  \n",
       "2                           0.773026                         1.672209  \n",
       "3                           0.926806                         2.132386  \n",
       "4                           1.043989                         1.794492  \n",
       "..                               ...                              ...  \n",
       "134                         0.918207                         1.803800  \n",
       "135                         0.823319                         2.056284  \n",
       "136                         1.001630                         1.818625  \n",
       "137                         1.033665                         1.835512  \n",
       "138                         0.983903                         1.750540  \n",
       "\n",
       "[139 rows x 144 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.merge(dfRad, left_on='PID', right_on='StudyPatientName')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfCoefResults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-54c941e87356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.colheader_justify'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfCoefResults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhide_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dfCoefResults' is not defined"
     ]
    }
   ],
   "source": [
    "pd.set_option('precision', 3)\n",
    "pd.set_option('display.colheader_justify','left')\n",
    "display(dfCoefResults.style.hide_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores and predicted class info\n",
    "y_pred_score_test = pipeline.predict_proba(XTest)[:, 1]\n",
    "y_pred_class_test = pipeline.predict(XTest)\n",
    "y_pred_score_test_reducedModel = pipeline_reducedModel.predict_proba(XTest_reducedModel)[:, 1]\n",
    "y_pred_class_test_reducedModel = pipeline_reducedModel.predict(XTest_reducedModel)\n",
    "y_pred_score_GG = pipeline.predict_proba(X_GG)[:, 1]\n",
    "y_pred_score_MG = pipeline.predict_proba(X_MG)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scores from main model\n",
    "testAUROC = roc_auc_score(yTest, y_pred_score_test)\n",
    "testAccuracy = accuracy_score(yTest, y_pred_class_test)\n",
    "testF1 = f1_score(yTest, y_pred_class_test)\n",
    "\n",
    "# test scores from reduced model\n",
    "test_reducedModel_AUROC = roc_auc_score(yTest, y_pred_score_test_reducedModel)\n",
    "test_reducedModel_Accuracy = accuracy_score(yTest, y_pred_class_test_reducedModel)\n",
    "test_reducedModel_F1 = f1_score(yTest, y_pred_class_test_reducedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the test performance metrics\n",
    "print('Principle model')\n",
    "print('AUCROC  (test)  = ' + str(np.round(testAUROC,3)))\n",
    "print('Accuracy (test) = ' + str(np.round(testAccuracy,3)))\n",
    "print('F1 (test)       = ' + str(np.round(testF1,3)))\n",
    "\n",
    "print('\\nReduced model using GleasonBinary, MeasurableECE, CapsularContactLength, IrregularContour, CapsularDisruption')\n",
    "print('AUCROC   = ' + str(np.round(test_reducedModel_AUROC,3)))\n",
    "print('Accuracy = ' + str(np.round(test_reducedModel_Accuracy,3)))\n",
    "print('F1       = ' + str(np.round(test_reducedModel_F1,3)))\n",
    "print(' ')\n",
    "\n",
    "data = np.stack((y_pred_score_MG, y_pred_score_GG), axis=1)\n",
    "iccScore = intraclass_correlation(data, \"twoway\", \"agreement\").value\n",
    "print('ICC comparing GG and MG scores  = ' + str(np.round(iccScore,3)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_thresholds(yTrue, yScore, thresholds):\n",
    "    tnArr, fpArr, fnArr, tpArr = [], [], [], []\n",
    "    nSamples = len(yTrue)\n",
    "    for thresh in thresholds:\n",
    "        tn, fp, fn, tp = confusion_matrix(yTrue, yScore>thresh).ravel()\n",
    "        tnArr.append(tn/nSamples)\n",
    "        fpArr.append(fp/nSamples)\n",
    "        fnArr.append(fn/nSamples)\n",
    "        tpArr.append(tp/nSamples)\n",
    "    return np.array(tnArr), np.array(fpArr), np.array(fnArr), np.array(tpArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot comparing scores\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.scatter(y_pred_score_GG, y_pred_score_MG, c=yMG_GG, s=10, cmap='bwr')\n",
    "plt.xlabel('Reader 1 score')\n",
    "plt.ylabel('Reader 2 score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot comparing ROCs\n",
    "fprGG, tprGG, _ = roc_curve(yMG_GG, y_pred_score_GG)\n",
    "fprMG, tprMG, _ = roc_curve(yMG_GG, y_pred_score_MG)\n",
    "fprTest, tprTest, _ = roc_curve(yTest, y_pred_score_test)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.plot(fprGG, tprGG,     label='Train, reader 1, AUROC = ' + str(np.round(roc_auc_score(yMG_GG, y_pred_score_GG),3)))\n",
    "plt.plot(fprMG, tprMG,     label='Train, reader 2, AUROC = ' + str(np.round(roc_auc_score(yMG_GG, y_pred_score_MG),3)))\n",
    "plt.plot(fprTest, tprTest, label='Test,  reader 1, AUROC = ' + str(np.round(roc_auc_score(yTest, y_pred_score_test),3)))\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.title('ROCs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots comparing TP, FP, Sensitivity etc.\n",
    "\n",
    "thresh = np.linspace(0, 1, 500)\n",
    "tnTest, fpTest, fnTest, tpTest = roc_curve_thresholds(yTest, y_pred_score_test, thresh)\n",
    "tnGG, fpGG, fnGG, tpGG = roc_curve_thresholds(yMG_GG, y_pred_score_GG, thresh)\n",
    "tnMG, fpMG, fnMG, tpMG = roc_curve_thresholds(yMG_GG, y_pred_score_MG, thresh)\n",
    "\n",
    "_, ax = plt.subplots(2,3, figsize=(16,12))\n",
    "ax = ax.ravel()\n",
    "\n",
    "ax[0].plot(thresh, tpGG/(tpGG + fnGG), label='Train, reader 1')\n",
    "ax[0].plot(thresh, tpMG/(tpMG + fnMG), label='Train, reader 2')\n",
    "ax[0].plot(thresh, tpTest/(tpTest + fnTest), label='Test,  reader 1')\n",
    "ax[0].set_title('True positive rate (Sensitivity)')\n",
    "ax[0].set_xlabel('Threshold')\n",
    "ax[0].set_ylabel('TPR')\n",
    "\n",
    "ax[1].plot(thresh, tnGG/(tnGG + fpGG), label='Train, reader 1')\n",
    "ax[1].plot(thresh, tnMG/(tnMG + fpMG), label='Train, reader 2')\n",
    "ax[1].plot(thresh, tnTest/(tnTest + fpTest), label='Test,  reader 1')\n",
    "ax[1].set_title('True negative rate (Specificity)')\n",
    "ax[1].set_xlabel('Threshold')\n",
    "ax[1].set_ylabel('TNR')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(thresh, fnGG/(fnGG + tpGG))\n",
    "ax[2].plot(thresh, fnMG/(fnMG + tpMG))\n",
    "ax[2].plot(thresh, fnTest/(fnTest + tpTest))\n",
    "ax[2].set_title('False negative rate')\n",
    "ax[2].set_xlabel('Threshold')\n",
    "ax[2].set_ylabel('FNR')\n",
    "\n",
    "ax[3].plot(thresh, fpGG/(fpGG + tnGG))\n",
    "ax[3].plot(thresh, fpMG/(fpMG + tnMG))\n",
    "ax[3].plot(thresh, fpTest/(fpTest + tnTest))\n",
    "ax[3].set_title('False positive rate')\n",
    "ax[3].set_xlabel('Threshold')\n",
    "ax[3].set_ylabel('FPR')\n",
    "\n",
    "ax[4].plot(thresh, (tnGG + tpGG)/(tpGG + fnGG + tnGG + fpGG))\n",
    "ax[4].plot(thresh, (tnMG + tpMG)/(tpMG + fnMG + tnMG + fpMG))\n",
    "ax[4].plot(thresh, (tnTest + tpTest)/(tpTest + fnTest + tnTest + fpTest))\n",
    "ax[4].set_title('Accuracy')\n",
    "ax[4].set_xlabel('Threshold')\n",
    "ax[4].set_ylabel('Accuracy')\n",
    "\n",
    "ax[5].plot(thresh, 2*tpGG/(2*tpGG + fnGG + fpGG))\n",
    "ax[5].plot(thresh, 2*tpMG/(2*tpMG + fnMG + fpMG))\n",
    "ax[5].plot(thresh, 2*tpTest/(2*tpTest + fnTest + fpTest))\n",
    "ax[5].set_title('F1')\n",
    "ax[5].set_xlabel('Threshold')\n",
    "ax[5].set_ylabel('F1')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].plot(thresh, tpGG/(tpGG + fnGG), label='Train, reader 1')\n",
    "ax[0].plot(thresh, tpMG/(tpMG + fnMG), label='Train, reader 2')\n",
    "ax[0].plot(thresh, tpTest/(tpTest + fnTest), label='Test,  reader 1')\n",
    "ax[0].set_title('True positive rate (Sensitivity)')\n",
    "ax[0].set_xlim([0, 0.5])\n",
    "ax[0].set_ylim([0.5, 1.05])\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Threshold')\n",
    "ax[0].set_ylabel('TPR')\n",
    "\n",
    "ax[1].plot(thresh, fnGG/(fnGG + tpGG), label='Train, reader 1')\n",
    "ax[1].plot(thresh, fnMG/(fnMG + tpMG), label='Train, reader 2')\n",
    "ax[1].plot(thresh, fnTest/(fnTest + tpTest), label='Test,  reader 1')\n",
    "ax[1].set_title('False negative rate')\n",
    "ax[1].set_xlim([-0.05, 0.5])\n",
    "ax[1].set_ylim([-0.05, 0.5])\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Threshold')\n",
    "ax[1].set_ylabel('FNR')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].plot(thresh, tpGG, label='Train, reader 1')\n",
    "ax[0].plot(thresh, tpMG, label='Train, reader 2')\n",
    "ax[0].plot(thresh, tpTest, label='Test,  reader 1')\n",
    "ax[0].set_xlim([0, 0.5])\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Threshold')\n",
    "ax[0].set_ylabel('TP')\n",
    "ax[0].set_title('True positives')\n",
    "\n",
    "ax[1].plot(thresh, fnGG, label='Train, reader 1')\n",
    "ax[1].plot(thresh, fnMG, label='Train, reader 2')\n",
    "ax[1].plot(thresh, fnTest, label='Test,  reader 1')\n",
    "ax[1].set_title('False negatives')\n",
    "ax[1].set_xlim([0, 0.5])\n",
    "#ax[1].set_ylim([0, 0.5])\n",
    "ax[1].legend()\n",
    "ax[0].set_xlabel('Threshold')\n",
    "ax[1].set_ylabel('FN')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
